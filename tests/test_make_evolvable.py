
# Generated by CodiumAI
import nntplib
import torch
import pytest
import torch.nn as nn
from agilerl.wrappers.make_evolvable import MakeEvolvable
from agilerl.wrappers.make_evolvable_old import MakeEvolvableOld
from agilerl.networks.evolvable_mlp import EvolvableMLP

class TwoArgCNN(nn.Module):
    def __init__(self):
        super(TwoArgCNN, self).__init__()

        # Define the convolutional layers
        self.conv1 = nn.Conv3d(in_channels=4, out_channels=16, kernel_size=(1,3,3), stride=4)    # W: 160, H: 210
        self.conv2 = nn.Conv3d(in_channels=16, out_channels=32, kernel_size=(1,3,3), stride=2)   # W:

        # Define the max-pooling layers
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        # Define fully connected layers
        self.fc1 = nn.Linear(304002 , 256)  
        self.fc2 = nn.Linear(256, 2)

        # Define activation function
        self.relu = nn.ReLU()

        # Define softmax for classification
        self.softmax = nn.Softmax(dim=1)
        self.tanh = nn.Tanh()


    def forward(self, x, xc):
        # Forward pass through convolutional layers
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))

        # Flatten the output for the fully connected layers
        x = x.view(x.size(0), -1)
        x = torch.cat([x, xc], dim=1)
        # Forward pass through fully connected layers
        x = self.tanh(self.fc1(x))
        x = self.fc2(x)

        # Apply softmax for classification
        x = self.softmax(x)

        return x

@pytest.fixture
def simple_mlp():
    network = nn.Sequential(
        nn.Linear(10, 20),
        nn.ReLU(),
        nn.Linear(20, 10),
        nn.ReLU(),
        nn.Linear(10, 1)
    )
    return network

@pytest.fixture
def simple_mlp_2():
    network = nn.Sequential(
        nn.Linear(10, 128),
        nn.ReLU(),
        nn.Linear(128, 128),
        nn.ReLU(),
        nn.Linear(128, 1)
    )
    return network

@pytest.fixture
def simple_cnn():
    network = nn.Sequential(
        nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # Input channels: 3 (for RGB images), Output channels: 16
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2),
        nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # Input channels: 16, Output channels: 32
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2),
        nn.Flatten(),  # Flatten the 2D feature map to a 1D vector
        nn.Linear(32 * 16 * 16, 128),  # Fully connected layer with 128 output features
        nn.ReLU(),
        nn.Linear(128, 1)  # Output layer with num_classes output features
    )
    return network 
    
@pytest.fixture
def two_arg_cnn():
    return TwoArgCNN()

@pytest.fixture
def device():
    return "cuda" if torch.cuda.is_available() else "cpu"

######### Test instantiation #########

# The class can be instantiated with all the required parameters and no errors occur.
@pytest.mark.parametrize("network, input_tensor, expected_result", [
    ('simple_mlp', torch.randn(1, 10), MakeEvolvable),
    ('simple_cnn', torch.randn(1, 3, 64, 64), MakeEvolvable)
] )
def test_instantiation_with_required_parameters(network, input_tensor, expected_result, request):
    network = request.getfixturevalue(network)
    evolvable_network = MakeEvolvable(network, input_tensor)
    assert isinstance(evolvable_network, expected_result)

@pytest.mark.skip
# The class can be instantiated with minimal parameters and default values are assigned correctly.
def test_instantiation_with_minimal_parameters():
    network = nn.Sequential(
        nn.Linear(10, 1)
    )
    input_tensor = torch.randn(1, 10)
    evolvable_network = MakeEvolvable(network, input_tensor)
    assert isinstance(evolvable_network, MakeEvolvable)

######### Test forward #########

@pytest.mark.parametrize("network, input_tensor, secondary_input_tensor, expected_result", [
    ('simple_mlp', torch.randn(1, 10), None, (1,1)),
    ('simple_cnn', torch.randn(1, 3, 64, 64), None, (1, 1)),
    ('two_arg_cnn', torch.randn(1, 4, 160, 210, 160), torch.randn(1,2), (1, 2)) 
] )
def test_forward_method(network, input_tensor, secondary_input_tensor, expected_result, request, device):
    network = request.getfixturevalue(network)
    if secondary_input_tensor is None:
        evolvable_network = MakeEvolvable(network, input_tensor, device=device)
        actual_output = evolvable_network.forward(input_tensor)
    else:
        evolvable_network = MakeEvolvable(network, input_tensor, secondary_input_tensor, device, extra_critic_dims=2)
        actual_output = network.forward(input_tensor, secondary_input_tensor)
    output_shape = actual_output.shape
    assert output_shape == expected_result

# The forward() method can handle different types of input tensors (e.g., numpy array, torch tensor).
def test_forward_method_with_different_input_types(simple_mlp):
    input_tensor = torch.randn(1, 10)
    numpy_array = input_tensor.numpy()
    evolvable_network = MakeEvolvable(simple_mlp, input_tensor)
    output1 = evolvable_network.forward(input_tensor)
    output2 = evolvable_network.forward(numpy_array)
    assert isinstance(output1, torch.Tensor)
    assert isinstance(output2, torch.Tensor)

# The forward() method can handle different types of normalization layers (e.g., BatchNorm2d, InstanceNorm3d).
def test_forward_with_different_normalization_layers():
    network = nn.Sequential(
        nn.Linear(10, 20),
        nn.ReLU(),
        nn.BatchNorm2d(20),
        nn.Linear(20, 10),
        nn.ReLU(),
        nn.Linear(10, 1)
    )
    input_tensor = torch.randn(1, 10)
    evolvable_network = MakeEvolvable(network, input_tensor)
    output = evolvable_network.forward(input_tensor)
    assert isinstance(output, torch.Tensor)


######### Test detect architecture function #########

nn.Sequential(
        nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # Input channels: 3 (for RGB images), Output channels: 16
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2),
        nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # Input channels: 16, Output channels: 32
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2),
        nn.Flatten(),  # Flatten the 2D feature map to a 1D vector
        nn.Linear(32 * 16 * 16, 128),  # Fully connected layer with 128 output features
        nn.ReLU(),
        nn.Linear(128, 1)  # Output layer with num_classes output features
    )

# Detects architecture of a neural network with convolutional layers and without normalization layers
def test_detect_architecture_conv_layers_no_norm(simple_cnn, device):
    input_tensor = torch.randn(1, 3, 64, 64)
    make_evolvable = MakeEvolvable(simple_cnn, input_tensor, secondary_input_tensor=None, device=device)

    assert make_evolvable.arch == "cnn"
    assert make_evolvable.in_channels == 3
    assert make_evolvable.channel_size == [16, 32]
    assert make_evolvable.kernel_size == [(3, 3), (3, 3)]
    assert make_evolvable.stride_size == [(1, 1), (1, 1)]
    assert make_evolvable.padding == [(1, 1), (1, 1)]
    assert make_evolvable.layer_indices == {'cnn': {'cnn_act': [0, 1], 'cnn_pool': [0, 1]}, 'mlp': {}}, f"{make_evolvable.layer_indices}"
    assert make_evolvable.has_conv_layers == True
    assert make_evolvable.conv_layer_type == "Conv2d"
    assert make_evolvable.cnn_activation == "ReLU"
    assert make_evolvable.cnn_norm == None
    assert make_evolvable.pooling == "MaxPool2d"
    assert make_evolvable.pooling_kernel == 2
    assert make_evolvable.mlp_activation == "ReLU"
    assert make_evolvable.mlp_output_activation == None
    assert make_evolvable.mlp_norm == None

# Detects architecture of a neural network with fully-connected layers and without activation layers
def test_detect_architecture_fc_layers_no_act(simple_mlp, device):

    input_tensor = torch.randn(1, 10)
    secondary_input_tensor = torch.randn(1, 10)

    make_evolvable = MakeEvolvable(simple_mlp, input_tensor, device=device)

    assert make_evolvable.arch == "mlp"
    assert make_evolvable.num_inputs == 10
    assert make_evolvable.num_outputs == 1
    assert make_evolvable.hidden_size == [20, 10]
    assert make_evolvable.mlp_output_activation == None
    assert make_evolvable.mlp_activation == "ReLU"
    assert make_evolvable.mlp_norm == None
    assert make_evolvable.layer_indices == {"cnn": {}, "mlp": {}}
    assert make_evolvable.has_conv_layers == False
    assert make_evolvable.conv_layer_type == None
    assert make_evolvable.cnn_activation == None
    assert make_evolvable.cnn_norm == None
    assert make_evolvable.pooling == None
    assert make_evolvable.pooling_kernel == None
    assert make_evolvable.mlp_norm == None
    assert make_evolvable.in_channels == None
    assert make_evolvable.channel_size == None
    assert make_evolvable.kernel_size == None
    assert make_evolvable.stride_size == None
    assert make_evolvable.padding == None

# Test if network after detect arch has the same arch as original network


######### Test create_mlp #########

######### Test create_cnn #########

######### Test create_nets #########

######### Test init_dict #########

######### Test add_mlp_layer #########
def test_add_mlp_layer(simple_mlp):
    input_tensor = torch.randn(1, 10)
    evolvable_network = MakeEvolvable(simple_mlp, input_tensor)
    initial_num_layers = len(evolvable_network.hidden_size)
    evolvable_network.add_mlp_layer()
    assert len(evolvable_network.hidden_size) == initial_num_layers + 1

######### Test remove_mlp_layer #########
def test_remove_mlp_layer(simple_mlp):
    input_tensor = torch.randn(1, 10)
    evolvable_network = MakeEvolvable(simple_mlp, input_tensor)
    initial_num_layers = len(evolvable_network.hidden_size)
    evolvable_network.remove_mlp_layer()
    assert len(evolvable_network.hidden_size) == initial_num_layers - 1

######### Test add_mlp_node #########
def test_add_mlp_node_fixed(simple_mlp):
    
    input_tensor = torch.randn(1, 10)
    evolvable_network = MakeEvolvable(simple_mlp, input_tensor)

    # Test adding a new node to a specific layer
    hidden_layer = 1
    numb_new_nodes = 8
    result = evolvable_network.add_mlp_node(hidden_layer, numb_new_nodes)

    # Check if the hidden layer and number of new nodes are updated correctly
    assert evolvable_network.hidden_size[hidden_layer] == 18
    assert result['hidden_layer'] == hidden_layer
    assert result['numb_new_nodes'] == numb_new_nodes

######### Test remove_mlp_node #########
def test_remove_mlp_node(simple_mlp_2):
   
    input_tensor = torch.randn(1, 10)
    evolvable_network = MakeEvolvable(simple_mlp_2, input_tensor)

    # Check the initial number of nodes in the hidden layers
    assert len(evolvable_network.hidden_size) == 2

    # Remove a node from the second hidden layer
    evolvable_network.remove_mlp_node(hidden_layer=1, numb_new_nodes=10)

    # Check that the number of nodes in the second hidden layer has decreased by 10
    assert evolvable_network.hidden_size[1] == 118

    # Remove a node from the first hidden layer
    evolvable_network.remove_mlp_node(hidden_layer=0, numb_new_nodes=5)

    # Check that the number of nodes in the first hidden layer has decreased by 5
    assert evolvable_network.hidden_size[0] == 123

######### Test add_cnn_layer #########
def test_add_cnn_layer(simple_cnn):
    input_tensor = torch.randn(1, 3, 64, 64)
    evolvable_network = MakeEvolvable(simple_cnn, input_tensor)

    # Check the initial number of layers
    assert len(evolvable_network.channel_size) == 2

    # Add a new CNN layer
    evolvable_network.add_cnn_layer()

    # Check if a new layer has been added
    assert len(evolvable_network.channel_size) == 3

######### Test change_cnn_kernel #########
def test_change_cnn_kernel(simple_cnn):
    
    input_tensor = torch.randn(1, 3, 64, 64)
    evolvable_network = MakeEvolvable(simple_cnn, input_tensor)

    # Check initial kernel sizes
    assert evolvable_network.kernel_size == [(3, 3), (3, 3)]

    # Change kernel size
    evolvable_network.change_cnn_kernel()

    while evolvable_network.kernel_size == [(3, 3), (3, 3)]:
        evolvable_network.change_cnn_kernel()

    # Check if kernel size has changed
    assert evolvable_network.kernel_size != [(3, 3), (3, 3)], evolvable_network.kernel_size

# Add in a test for 3d convolutions

######### Test add_cnn_channel #########

######### Test recreate_nets #########
def test_recreate_nets_parameters_preserved(simple_mlp):
    input_tensor = torch.randn(1, 10)
    evolvable_network = MakeEvolvable(simple_mlp, input_tensor)

    value_net = evolvable_network.value_net
    value_net_dict = dict(value_net.named_parameters())

    # Modify the architecture
    evolvable_network.hidden_size += [evolvable_network.hidden_size[-1]]

    
    evolvable_network.recreate_nets()
    new_value_net = evolvable_network.value_net

    for key, param in new_value_net.named_parameters():
        if key in value_net_dict.keys():
            assert(torch.equal(param, value_net_dict[key]))

def test_recreate_nets_parameters_shrink_preserved(device):
    network = nn.Sequential(nn.Linear(4,32), nn.ReLU(), nn.Linear(32,32), nn.ReLU(), nn.Linear(32,2))

    input_tensor = torch.randn(1, 4)
    evolvable_network = MakeEvolvable(network, input_tensor, device=device)

    value_net = evolvable_network.value_net
    value_net_dict = dict(value_net.named_parameters())

    print(evolvable_network.hidden_size)

    print(evolvable_network)

    # Modify the architecture
    evolvable_network.hidden_size = evolvable_network.hidden_size[:-1]
    print(evolvable_network.hidden_size)

    print(evolvable_network)
    evolvable_network.recreate_nets(shrink_params=True)
    new_value_net = evolvable_network.value_net

    for key, param in new_value_net.named_parameters():
        if key in value_net_dict.keys():
            print("----------------", key, "-------------------")
            print(param, value_net_dict[key])
            torch.testing.assert_close(param, value_net_dict[key])


# def test_recreate_nets_net_config(device):
    
#     network = EvolvableMLP(num_inputs=6,
#                            num_outputs=4,
#                            hidden_size=[32, 32],
#                            device=device)
    
#     value_net = network.net
#     value_net_dict = dict(value_net.named_parameters())

#     network.add_mlp_layer()

#     new_value_net = network.net
    
#     for key, param in new_value_net.named_parameters():
#         if key in value_net_dict.keys():
#             assert(torch.equal(param, value_net_dict[key]))


######### Test clone #########

# The clone() method successfully creates a deep copy of the model.
@pytest.mark.parametrize("network, input_tensor, secondary_input_tensor", [
    ('simple_mlp', torch.randn(1, 10), None),
    ('simple_cnn', torch.randn(1, 3, 64, 64), None,),
    ('two_arg_cnn', torch.randn(1, 4, 160, 210, 160),  torch.randn(1,2)) 
] )
def test_clone_method_with_equal_state_dicts(network, input_tensor, secondary_input_tensor, request, device):
    network = request.getfixturevalue(network)
    if secondary_input_tensor is None:
        evolvable_network = MakeEvolvable(network, input_tensor, device=device)
    else:
        evolvable_network = MakeEvolvable(network, input_tensor, secondary_input_tensor, device=device, extra_critic_dims=2)
    clone_network = evolvable_network.clone()
    assert isinstance(clone_network, MakeEvolvable)
    assert str(evolvable_network.state_dict()) == str(clone_network.state_dict())

# Test clone after modifying arch to see if original layers still have same params


######### Test shrink_preserve_parameters #########











#### Detect archtiecture tests
# @pytest.mark.parametrize("network, input_tensor, secondary_input_tensor", [
#     ('simple_mlp', torch.randn(1, 10), None),
#     ('simple_cnn', torch.randn(1, 3, 64, 64), None,),
#     ('two_arg_cnn', torch.randn(1, 4, 160, 210, 160),  torch.randn(1,2)) 
# ] )
# def test_detect_architecture_basic_layers(network, input_tensor, secondary_input_tensor, request, device):
#     network = request.getfixturevalue(network)
#     if secondary_input_tensor is None:
#         evolvable_network = MakeEvolvable(network, input_tensor, device=device)
#     else:
#         evolvable_network = MakeEvolvable(network, input_tensor, secondary_input_tensor, device=device, extra_critic_dims=2)
#     for _ in zip(network.named_parameters(), evolvable_network.named_parameters())




# # The add_mlp_node() method successfully adds a new node to a specific layer in the MLP network.


# # The change_cnn_kernel() method successfully changes the kernel size of a specific layer in the CNN network.


# # The remove_mlp_node() method successfully removes a node from a specific layer in the MLP network.


# # The forward() method can handle different types of activation functions (e.g., Tanh, ReLU, Softmax).
# def test_forward_method_with_different_activation_functions(self):
#     network = nn.Sequential(
#         nn.Linear(10, 20),
#         nn.ReLU(),
#         nn.Linear(20, 10),
#         nn.ReLU(),
#         nn.Linear(10, 1)
#     )
#     input_tensor = torch.randn(1, 10)
#     evolvable_network = MakeEvolvable(network, input_tensor)

#     # Test with Tanh activation
#     evolvable_network.mlp_activation = 'Tanh'
#     output = evolvable_network.forward(input_tensor)
#     assert isinstance(output, torch.Tensor)

#     # Test with ReLU activation
#     evolvable_network.mlp_activation = 'ReLU'
#     output = evolvable_network.forward(input_tensor)
#     assert isinstance(output, torch.Tensor)

#     # Test with Softmax activation
#     evolvable_network.mlp_activation = 'Softmax'
#     output = evolvable_network.forward(input_tensor)
#     assert isinstance(output, torch.Tensor)

# # The forward() method can handle different types of convolutional layers (e.g., Conv2d, Conv3d).
# def test_forward_method_with_different_conv_layers(self):
#     network = nn.Sequential(
#         nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
#         nn.ReLU(),
#         nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
#         nn.ReLU(),
#         nn.Linear(128, 10)
#     )
#     input_tensor = torch.randn(1, 3, 32, 32)
#     evolvable_network = MakeEvolvable(network, input_tensor)
#     output = evolvable_network.forward(input_tensor)
#     assert output.shape == (1, 10)
#     assert isinstance(output, torch.Tensor)
#     assert output.dtype == torch.float32

# # The recreate_nets() method successfully recreates the feature and value networks with updated architecture.
# def test_recreate_nets_fixed(self):
#     network = nn.Sequential(
#         nn.Linear(10, 20),
#         nn.ReLU(),
#         nn.Linear(20, 10),
#         nn.ReLU(),
#         nn.Linear(10, 1)
#     )
#     input_tensor = torch.randn(1, 10)
#     evolvable_network = MakeEvolvable(network, input_tensor)

#     # Modify the architecture
#     evolvable_network.hidden_size = [32, 64, 128]
#     evolvable_network.recreate_nets()

#     # Check if the value network has been recreated with the updated architecture
#     assert isinstance(evolvable_network.value_net, nn.Sequential)
#     assert len(evolvable_network.value_net) == 4
#     assert isinstance(evolvable_network.value_net[0], nn.Linear)
#     assert isinstance(evolvable_network.value_net[1], nn.ReLU)
#     assert isinstance(evolvable_network.value_net[2], nn.Linear)
#     assert isinstance(evolvable_network.value_net[3], nn.Linear)

# # The add_cnn_channel() method successfully adds a new channel to a specific layer in the CNN network.
# def test_add_cnn_channel(self):
#     network = nn.Sequential(
#         nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),
#         nn.ReLU(),
#         nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),
#         nn.ReLU(),
#         nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
#         nn.ReLU()
#     )
#     input_tensor = torch.randn(1, 3, 32, 32)
#     evolvable_network = MakeEvolvable(network, input_tensor)

#     # Check the initial number of channels in the first convolutional layer
#     assert evolvable_network.channel_size[0] == 16

#     # Add a new channel to the first convolutional layer
#     evolvable_network.add_cnn_channel(hidden_layer=0, numb_new_channels=8)

#     # Check if a new channel has been added to the first convolutional layer
#     assert evolvable_network.channel_size[0] == 24

#     # Add a new channel to the second convolutional layer
#     evolvable_network.add_cnn_channel(hidden_layer=1, numb_new_channels=16)

#     # Check if a new channel has been added to the second convolutional layer
#     assert evolvable_network.channel_size[1] == 48

#     # Add a new channel to the third convolutional layer
#     evolvable_network.add_cnn_channel(hidden_layer=2, numb_new_channels=32)

#     # Check if a new channel has been added to the third convolutional layer
#     assert evolvable_network.channel_size[2] == 96

# # The init_dict property returns a dictionary with all the necessary information to recreate the model.
# def test_init_dict_property(self):
#     network = nn.Sequential(
#         nn.Linear(10, 20),
#         nn.ReLU(),
#         nn.Linear(20, 10),
#         nn.ReLU(),
#         nn.Linear(10, 1)
#     )
#     input_tensor = torch.randn(1, 10)
#     evolvable_network = MakeEvolvable(network, input_tensor)
#     init_dict = evolvable_network.init_dict
#     assert isinstance(init_dict, dict)
#     assert init_dict['network'] is None
#     assert init_dict['input_tensor'] is input_tensor
#     assert init_dict['num_inputs'] == 10
#     assert init_dict['num_outputs'] == 1
#     assert init_dict['hidden_size'] == [20, 10]
#     assert init_dict['mlp_activation'] == 'ReLU'
#     assert init_dict['mlp_output_activation'] is None
#     assert init_dict['layer_indices'] == {'cnn': {}, 'mlp': {}}
#     assert init_dict['mlp_norm'] is None
#     assert init_dict['cnn_norm'] is None
#     assert init_dict['device'] == 'cpu'
#     assert init_dict['accelerator'] is None
#     assert init_dict['in_channels'] is None
#     assert init_dict['channel_size'] is None
#     assert init_dict['kernel_size'] is None
#     assert init_dict['stride_size'] is None
#     assert init_dict['padding'] is None
#     assert init_dict['pooling'] is None
#     assert init_dict['pooling_kernel'] is None
#     assert init_dict['cnn_activation'] is None
#     assert init_dict['conv_layer_type'] is None
#     assert init_dict['extra_critic_dims'] is None
#     assert init_dict['output_vanish'] is False
#     assert init_dict['init_layers'] is False
#     assert init_dict['has_conv_layer'] is False
#     assert init_dict['arch'] == 'mlp'

# # The forward() method can handle different types of pooling layers (e.g., MaxPool2d, AvgPool3d).
# def test_forward_with_different_pooling_layers(self):
#     network = nn.Sequential(
#         nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size=2, stride=2),
#         nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),
#         nn.ReLU(),
#         nn.AvgPool2d(kernel_size=2, stride=2),
#         nn.Flatten(),
#         nn.Linear(32 * 7 * 7, 10),
#         nn.Softmax(dim=-1)
#     )
#     input_tensor = torch.randn(1, 3, 28, 28)
#     evolvable_network = MakeEvolvable(network, input_tensor)

#     # Test MaxPool2d
#     x = torch.randn(1, 3, 28, 28)
#     output = evolvable_network.forward(x)
#     assert output.shape == (1, 10)

#     # Test AvgPool2d
#     x = torch.randn(1, 3, 28, 28)
#     output = evolvable_network.forward(x)
#     assert output.shape == (1, 10)

#     # Test MaxPool3d
#     network = nn.Sequential(
#         nn.Conv3d(3, 16, kernel_size=3, stride=1, padding=1),
#         nn.ReLU(),
#         nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),
#         nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1),
#         nn.ReLU(),
#         nn.AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),
#         nn.Flatten(),
#         nn.Linear(32 * 7 * 7 * 7, 10),
#         nn.Softmax(dim=-1)
#     )
#     input_tensor = torch.randn(1, 3, 28, 28, 28)
#     evolvable_network = MakeEvolvable(network, input_tensor)

#     x = torch.randn(1, 3, 28, 28, 28)
#     output = evolvable_network.forward(x)
#     assert output.shape == (1, 10)

#     # Test AvgPool3d
#     x = torch.randn(1, 3, 28, 28, 28)
#     output = evolvable_network.forward(x)
#     assert output.shape == (1, 10)