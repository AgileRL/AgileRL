defaults:
  - model: per_token_iql
  - dataset@train_dataset: human_dataset
  - dataset@eval_dataset: human_dataset
  - evaluator: iql_evaluator
  - _self_

train_dataset:
  cache_id: d_train
  file_path: data/wordle/random_human_tweet_data_200.json
  use_true_word: false
  index_file: data/wordle/human_train_idxs.json

eval_dataset:
  cache_id: d_eval
  file_path: data/wordle/random_human_tweet_data_200.json
  use_true_word: false
  index_file: data/wordle/human_eval_idxs.json

model:
  name: per_token_iql
  index: 0
  batch_size: 32
  lr: 1e-5
  alpha: 0.005
  beta: 0.0
  gamma: 0.99
  tau: 0.7
  mutation: null
  transition_weight: 0.0
  clip_weight: null
  value_max: null
  value_min: null
  detach_v: false
  detach_q: false
  detach_pi: false
  double_q: true
  per_token: true
  exp_weights: true
  dm_margin: 0.0
  cql_temp: 1.0
  weight_decay: 0.0
  dataset:
    name: wordle_human_dataset
    cache_id: d_train
  net_config: {
                'arch': 'gpt',
                'vocab_size': 35,
                'n_layer': 12,
                'n_embd': 768,
                'n_head': 12,
                'dim_feedfwd': 3072,
                'block_size': 1024,
                'activation': 'gelu',
                'dropout': 0.1,
                'layer_norm_eps': 1e-5,
                'min_layers': 8,
                'max_layers': 16,
                'bias': True,
              }
  load:
    checkpoint_path: null
    strict_load: false
    gpt_pretrained: true
    gpt_model_type: gpt2
    gpt_checkpoint_path: ../Implicit-Language-Q-Learning/outputs/wordle_tweet_bc_test1/model_327679.pkl

evaluator:
  env:
    vocab:
      name: vocab
      vocab_path: data/wordle/word_lists/wordle_official_200.txt
      cache_path: null
      fill_cache: true
  verbose: true
  kind: beam
  generation_kwargs:
    max_generation_len: 6
    beam_width: 1
    temp: 1.0
    top_k: null
    top_p: null
    exp_adv: true
    adv_weight: 16.0
    adv_clip: 0.0
    include_logits: true
    include_adv: true
    # num_generations: 1
    # rerank_log_prob_weight: 0.0
    # rerank_advantage_weight: 1.0

train:
  save_checkpoint_dir: outputs/wordle/wordle_iql_official_test1/
  optim_state_path: null
  epochs: 1
  dataloader_workers: 1
  grad_accum_steps: 16
  log_every: 256
  eval_every: 2 #1024
  save_every: 16384
  max_checkpoints: 1
  eval_bsize: 16
  eval_batches: 1
  weight_decay: 0.00
  hard_update_every: null
  max_steps: null
  loss:
    v_loss_weight: 1.0
    q_loss_weight: 1.0
    awac_weight: 0.0
    cql_loss_weight: 1e-4
    dm_loss_weight: 0.0
    mc_returns: false

wandb:
  use_wandb: false
  wandb_project: wordle_iql
