---
# Define initial hyperparameters
INIT_HP:
    ENV_NAME: pettingzoo.mpe.simple_speaker_listener_v4   # Gym environment name
    ALGO: MATD3                                         # Algorithm
  # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]
    CHANNELS_LAST: false
    MAX_ACTION: 1              # Maximum action
    MIN_ACTION: -1             # Minimum action
    BATCH_SIZE: 1024           # Batch size
    LR_ACTOR: 0.001            # Actor learning rate
    LR_CRITIC: 0.01            # Critic earning rate
    EPISODES: 6000             # Max no. episodes
    TARGET_SCORE: 100          # Early training stop at avg score of last 100 episodes
    GAMMA: 0.95                # Discount factor
    MEMORY_SIZE: 1_000_000        # Max memory buffer size
    POLICY_FREQ: 2             # Policy update frequency
    LEARN_STEP: 100            # Learning frequency
    TAU: 0.01                  # For soft update of target parameters
    TOURN_SIZE: 2              # Tournament size
    ELITISM: true              # Elitism in tournament selection
    POP_SIZE: 6                # Population size
    EVO_EPOCHS: 20             # Evolution frequency
    WANDB: true               # Log with Weights and Biases

MUTATION_PARAMS:
    NO_MUT: 0.4                            # No mutation
    ARCH_MUT: 0.2                          # Architecture mutation
    NEW_LAYER: 0.2                         # New layer mutation
    PARAMS_MUT: 0.2                        # Network parameters mutation
    ACT_MUT: 0.2                             # Activation layer mutation
    RL_HP_MUT: 0.2                         # Learning HP mutation
  # Learning HPs to choose from
    RL_HP_SELECTION: [lr, batch_size, learn_step]
    MUT_SD: 0.1                            # Mutation strength
    RAND_SEED: 42                          # Random seed
    MIN_LR: 0.0001                         # Define max and min limits for mutating RL hyperparams
    MAX_LR: 0.01
    MIN_LEARN_STEP: 20
    MAX_LEARN_STEP: 200
    MIN_BATCH_SIZE: 8
    MAX_BATCH_SIZE: 1024


NET_CONFIG: {arch: mlp, hidden_size: [64, 64], min_hidden_layers: 1, max_hidden_layers: 3, min_mlp_nodes: 64, max_mlp_nodes: 500}

DISTRIBUTED_TRAINING: false
