{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaimesabal/.pyenv/versions/agilerl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gymnasium import spaces\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "from agilerl.modules.configs import MlpNetConfig, CnnNetConfig, MultiInputNetConfig\n",
    "from agilerl.networks.q_networks import QNetwork, RainbowQNetwork\n",
    "from agilerl.networks.value_functions import ValueFunction\n",
    "from agilerl.networks.actors import StochasticActor, DeterministicActor\n",
    "\n",
    "from agilerl.algorithms.dqn import DQN\n",
    "from agilerl.utils.utils import create_population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.helper_functions import generate_dict_or_tuple_space\n",
    "\n",
    "img_space = spaces.Box(low=0, high=255, shape=(4, 84, 84))\n",
    "vec_space = spaces.Box(low=-1, high=1, shape=(4,), dtype='float32')\n",
    "dict_space = spaces.Dict({'img': img_space, 'vec': vec_space})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "img_config = CnnNetConfig(\n",
    "    channel_size=[16],\n",
    "    kernel_size=[4],\n",
    "    stride_size=[1],\n",
    ")\n",
    "vec_config = MlpNetConfig(\n",
    "    hidden_size=[64],\n",
    ")\n",
    "multi_input_config = MultiInputNetConfig(\n",
    "    channel_size=[8, 8, 8],\n",
    "    kernel_size=[2, 2, 2],\n",
    "    stride_size=[2, 2, 2],\n",
    "    hidden_size=[32, 32, 32],\n",
    "    vector_space_mlp=False\n",
    ")\n",
    "\n",
    "actor = RainbowQNetwork(\n",
    "    observation_space=img_space,\n",
    "    action_space=spaces.Discrete(4),\n",
    "    support=torch.linspace(-10, 10, 51),\n",
    "    encoder_config=img_config,\n",
    "    latent_dim=64,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_equal_params_ind(before_ind, mutated_ind):\n",
    "    before_dict = dict(before_ind.named_parameters())\n",
    "    after_dict = mutated_ind.named_parameters()\n",
    "    _not_eq = []\n",
    "    for key, param in after_dict:\n",
    "        if key in before_dict:\n",
    "            old_param = before_dict[key]\n",
    "            old_size = old_param.data.size()\n",
    "            new_size = param.data.size()\n",
    "            if old_size == new_size:\n",
    "                # If the sizes are the same, just copy the parameter\n",
    "                param.data = old_param.data\n",
    "            elif \"norm\" not in key:\n",
    "                # Create a slicing index to handle tensors with varying sizes\n",
    "                slice_index = tuple(slice(0, min(o, n)) for o, n in zip(old_size[:2], new_size[:2]))\n",
    "                # assert (\n",
    "                #     torch.all(torch.eq(param.data[slice_index], old_param.data[slice_index]))), \\\n",
    "                #     f\"Parameter {key} not equal after mutation {mutated_ind.last_mutation_attr}:\\n{param.data[slice_index]}\\n{old_param.data[slice_index]}\"\n",
    "                if not torch.all(torch.eq(param.data[slice_index], old_param.data[slice_index])):\n",
    "                    _not_eq.append(key)\n",
    "    \n",
    "    print(_not_eq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agilerl.modules.bert import EvolvableBERT\n",
    "\n",
    "mod = EvolvableBERT([12], [12], device=device)\n",
    "new_mod = mod.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mod.add_node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.encoder.state_dict()['bert_encoder_layer_0.linear1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mod.encoder.state_dict()['bert_encoder_layer_0.linear1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agilerl.modules.cnn import EvolvableCNN\n",
    "from agilerl.hpo.mutation import Mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "with open('configs/training/ppo.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "vector_actions = spaces.Box(low=-1, high=1, shape=(4,), dtype='float32')\n",
    "discrete_actions = spaces.Discrete(4)\n",
    "\n",
    "# accelerator = Accelerator()\n",
    "INIT_HP = config[\"INIT_HP\"]\n",
    "INIT_HP['AGENT_IDS'] = [f'agent_{i}' for i in range(4)]\n",
    "n_agents = 4\n",
    "agent_pop = create_population(\n",
    "    algo=INIT_HP[\"ALGO\"],\n",
    "    observation_space=img_space,\n",
    "    action_space=discrete_actions,\n",
    "    net_config={'encoder_config': img_config},\n",
    "    INIT_HP=INIT_HP,\n",
    "    population_size=INIT_HP[\"POP_SIZE\"],\n",
    "    num_envs=INIT_HP[\"NUM_ENVS\"],\n",
    "    device=device,\n",
    "    # accelerator=accelerator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StochasticActor(\n",
       "   (encoder): EvolvableCNN(\n",
       "     (model): Sequential(\n",
       "       (encoder_conv_layer_1): Conv2d(4, 16, kernel_size=(4, 4), stride=(1, 1))\n",
       "       (encoder_layer_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (encoder_activation_1): ReLU()\n",
       "       (encoder_flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "       (encoder_linear_output): Linear(in_features=104976, out_features=32, bias=True)\n",
       "       (encoder_output_activation): ReLU()\n",
       "     )\n",
       "   )\n",
       "   (head_net): EvolvableDistribution(\n",
       "     (_wrapped): EvolvableMLP(\n",
       "       (model): Sequential(\n",
       "         (actor_linear_layer_1): Linear(in_features=32, out_features=16, bias=True)\n",
       "         (actor_layer_norm_1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "         (actor_activation_1): ReLU()\n",
       "         (actor_linear_layer_output): Linear(in_features=16, out_features=4, bias=True)\n",
       "         (actor_activation_output): Softmax(dim=-1)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ValueFunction(\n",
       "   (encoder): EvolvableCNN(\n",
       "     (model): Sequential(\n",
       "       (encoder_conv_layer_1): Conv2d(4, 16, kernel_size=(4, 4), stride=(1, 1))\n",
       "       (encoder_layer_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (encoder_activation_1): ReLU()\n",
       "       (encoder_flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "       (encoder_linear_output): Linear(in_features=104976, out_features=32, bias=True)\n",
       "       (encoder_output_activation): ReLU()\n",
       "     )\n",
       "   )\n",
       "   (head_net): EvolvableMLP(\n",
       "     (model): Sequential(\n",
       "       (value_linear_layer_1): Linear(in_features=32, out_features=16, bias=True)\n",
       "       (value_layer_norm_1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "       (value_activation_1): ReLU()\n",
       "       (value_linear_layer_output): Linear(in_features=16, out_features=1, bias=True)\n",
       "       (value_activation_output): Identity()\n",
       "     )\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_pop[0].optimizer.networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_net.add_node\n",
      "RainbowQNetwork(\n",
      "  (encoder): EvolvableCNN(\n",
      "    (model): Sequential(\n",
      "      (encoder_conv_layer_1): Conv2d(4, 16, kernel_size=(4, 4), stride=(1, 1))\n",
      "      (encoder_layer_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (encoder_activation_1): ReLU()\n",
      "      (encoder_flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (encoder_linear_output): Linear(in_features=104976, out_features=32, bias=True)\n",
      "      (encoder_output_activation): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (head_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (value_linear_layer_1): NoisyLinear(in_features=32, out_features=80)\n",
      "      (value_layer_norm_1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
      "      (value_activation_1): ReLU()\n",
      "      (value_linear_layer_output): NoisyLinear(in_features=80, out_features=51)\n",
      "      (value_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (advantage_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (advantage_linear_layer_1): NoisyLinear(in_features=32, out_features=80)\n",
      "      (advantage_layer_norm_1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
      "      (advantage_activation_1): ReLU()\n",
      "      (advantage_linear_layer_output): NoisyLinear(in_features=80, out_features=204)\n",
      "      (advantage_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "RainbowQNetwork(\n",
      "  (encoder): EvolvableCNN(\n",
      "    (model): Sequential(\n",
      "      (encoder_conv_layer_1): Conv2d(4, 16, kernel_size=(4, 4), stride=(1, 1))\n",
      "      (encoder_layer_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (encoder_activation_1): ReLU()\n",
      "      (encoder_flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (encoder_linear_output): Linear(in_features=104976, out_features=32, bias=True)\n",
      "      (encoder_output_activation): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (head_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (value_linear_layer_1): NoisyLinear(in_features=32, out_features=80)\n",
      "      (value_layer_norm_1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
      "      (value_activation_1): ReLU()\n",
      "      (value_linear_layer_output): NoisyLinear(in_features=80, out_features=51)\n",
      "      (value_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (advantage_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (advantage_linear_layer_1): NoisyLinear(in_features=32, out_features=80)\n",
      "      (advantage_layer_norm_1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
      "      (advantage_activation_1): ReLU()\n",
      "      (advantage_linear_layer_output): NoisyLinear(in_features=80, out_features=204)\n",
      "      (advantage_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "head_net.add_layer\n",
      "RainbowQNetwork(\n",
      "  (encoder): EvolvableCNN(\n",
      "    (model): Sequential(\n",
      "      (encoder_conv_layer_1): Conv2d(4, 16, kernel_size=(4, 4), stride=(1, 1))\n",
      "      (encoder_layer_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (encoder_activation_1): ReLU()\n",
      "      (encoder_flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (encoder_linear_output): Linear(in_features=104976, out_features=32, bias=True)\n",
      "      (encoder_output_activation): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (head_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (value_linear_layer_1): NoisyLinear(in_features=32, out_features=64)\n",
      "      (value_layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (value_activation_1): ReLU()\n",
      "      (value_linear_layer_2): NoisyLinear(in_features=64, out_features=64)\n",
      "      (value_layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (value_activation_2): ReLU()\n",
      "      (value_linear_layer_output): NoisyLinear(in_features=64, out_features=51)\n",
      "      (value_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (advantage_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (advantage_linear_layer_1): NoisyLinear(in_features=32, out_features=64)\n",
      "      (advantage_layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (advantage_activation_1): ReLU()\n",
      "      (advantage_linear_layer_2): NoisyLinear(in_features=64, out_features=64)\n",
      "      (advantage_layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (advantage_activation_2): ReLU()\n",
      "      (advantage_linear_layer_output): NoisyLinear(in_features=64, out_features=204)\n",
      "      (advantage_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "RainbowQNetwork(\n",
      "  (encoder): EvolvableCNN(\n",
      "    (model): Sequential(\n",
      "      (encoder_conv_layer_1): Conv2d(4, 16, kernel_size=(4, 4), stride=(1, 1))\n",
      "      (encoder_layer_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (encoder_activation_1): ReLU()\n",
      "      (encoder_flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (encoder_linear_output): Linear(in_features=104976, out_features=32, bias=True)\n",
      "      (encoder_output_activation): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (head_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (value_linear_layer_1): NoisyLinear(in_features=32, out_features=64)\n",
      "      (value_layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (value_activation_1): ReLU()\n",
      "      (value_linear_layer_2): NoisyLinear(in_features=64, out_features=64)\n",
      "      (value_layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (value_activation_2): ReLU()\n",
      "      (value_linear_layer_output): NoisyLinear(in_features=64, out_features=51)\n",
      "      (value_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (advantage_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (advantage_linear_layer_1): NoisyLinear(in_features=32, out_features=64)\n",
      "      (advantage_layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (advantage_activation_1): ReLU()\n",
      "      (advantage_linear_layer_2): NoisyLinear(in_features=64, out_features=64)\n",
      "      (advantage_layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (advantage_activation_2): ReLU()\n",
      "      (advantage_linear_layer_output): NoisyLinear(in_features=64, out_features=204)\n",
      "      (advantage_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "encoder.add_layer\n",
      "RainbowQNetwork(\n",
      "  (encoder): EvolvableCNN(\n",
      "    (model): Sequential(\n",
      "      (encoder_conv_layer_1): Conv2d(4, 16, kernel_size=(4, 4), stride=(1, 1))\n",
      "      (encoder_layer_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (encoder_activation_1): ReLU()\n",
      "      (encoder_conv_layer_2): Conv2d(16, 16, kernel_size=(6, 6), stride=(1, 1))\n",
      "      (encoder_layer_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (encoder_activation_2): ReLU()\n",
      "      (encoder_flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (encoder_linear_output): Linear(in_features=92416, out_features=32, bias=True)\n",
      "      (encoder_output_activation): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (head_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (value_linear_layer_1): NoisyLinear(in_features=32, out_features=64)\n",
      "      (value_layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (value_activation_1): ReLU()\n",
      "      (value_linear_layer_output): NoisyLinear(in_features=64, out_features=51)\n",
      "      (value_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (advantage_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (advantage_linear_layer_1): NoisyLinear(in_features=32, out_features=64)\n",
      "      (advantage_layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (advantage_activation_1): ReLU()\n",
      "      (advantage_linear_layer_output): NoisyLinear(in_features=64, out_features=204)\n",
      "      (advantage_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "RainbowQNetwork(\n",
      "  (encoder): EvolvableCNN(\n",
      "    (model): Sequential(\n",
      "      (encoder_conv_layer_1): Conv2d(4, 16, kernel_size=(4, 4), stride=(1, 1))\n",
      "      (encoder_layer_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (encoder_activation_1): ReLU()\n",
      "      (encoder_conv_layer_2): Conv2d(16, 16, kernel_size=(6, 6), stride=(1, 1))\n",
      "      (encoder_layer_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (encoder_activation_2): ReLU()\n",
      "      (encoder_flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (encoder_linear_output): Linear(in_features=92416, out_features=32, bias=True)\n",
      "      (encoder_output_activation): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (head_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (value_linear_layer_1): NoisyLinear(in_features=32, out_features=64)\n",
      "      (value_layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (value_activation_1): ReLU()\n",
      "      (value_linear_layer_output): NoisyLinear(in_features=64, out_features=51)\n",
      "      (value_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (advantage_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (advantage_linear_layer_1): NoisyLinear(in_features=32, out_features=64)\n",
      "      (advantage_layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (advantage_activation_1): ReLU()\n",
      "      (advantage_linear_layer_output): NoisyLinear(in_features=64, out_features=204)\n",
      "      (advantage_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "add_latent_node\n",
      "RainbowQNetwork(\n",
      "  (encoder): EvolvableCNN(\n",
      "    (model): Sequential(\n",
      "      (encoder_conv_layer_1): Conv2d(4, 16, kernel_size=(4, 4), stride=(1, 1))\n",
      "      (encoder_layer_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (encoder_activation_1): ReLU()\n",
      "      (encoder_flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (encoder_linear_output): Linear(in_features=104976, out_features=48, bias=True)\n",
      "      (encoder_output_activation): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (head_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (value_linear_layer_1): NoisyLinear(in_features=48, out_features=64)\n",
      "      (value_layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (value_activation_1): ReLU()\n",
      "      (value_linear_layer_output): NoisyLinear(in_features=64, out_features=4)\n",
      "      (value_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (advantage_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (advantage_linear_layer_1): NoisyLinear(in_features=48, out_features=64)\n",
      "      (advantage_layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (advantage_activation_1): ReLU()\n",
      "      (advantage_linear_layer_output): NoisyLinear(in_features=64, out_features=204)\n",
      "      (advantage_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "RainbowQNetwork(\n",
      "  (encoder): EvolvableCNN(\n",
      "    (model): Sequential(\n",
      "      (encoder_conv_layer_1): Conv2d(4, 16, kernel_size=(4, 4), stride=(1, 1))\n",
      "      (encoder_layer_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (encoder_activation_1): ReLU()\n",
      "      (encoder_flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (encoder_linear_output): Linear(in_features=104976, out_features=48, bias=True)\n",
      "      (encoder_output_activation): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (head_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (value_linear_layer_1): NoisyLinear(in_features=48, out_features=64)\n",
      "      (value_layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (value_activation_1): ReLU()\n",
      "      (value_linear_layer_output): NoisyLinear(in_features=64, out_features=51)\n",
      "      (value_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (advantage_net): EvolvableMLP(\n",
      "    (model): Sequential(\n",
      "      (advantage_linear_layer_1): NoisyLinear(in_features=48, out_features=64)\n",
      "      (advantage_layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (advantage_activation_1): ReLU()\n",
      "      (advantage_linear_layer_output): NoisyLinear(in_features=64, out_features=204)\n",
      "      (advantage_activation_output): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">14</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>device=device,)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span>new_population = [agent.clone(wrap=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> agent <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> agent_pop]                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>14 mutated_population = <span style=\"font-weight: bold; text-decoration: underline\">mutations.mutation(new_population, </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">True</span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 # print([ind.mut for ind in mutated_population])</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jaimesabal/projects/AgileRL/agilerl/hpo/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">mutation.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">484</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">mutation</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">481 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(eval_offspring)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">482 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Reinitialize shared with frozen weights due to </span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">483 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># potential mutation in architecture</span>                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>484 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>ind_shared = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">.reinit_from_mutated(eval_offspring)</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">485 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">486 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.accelerator <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">487 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   </span>ind_shared = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.to_device(ind_shared)                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jaimesabal/projects/AgileRL/agilerl/hpo/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">mutation.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">399</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">reinit_from_mutated</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">396 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>offspring.init_dict                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">397 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">398 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(ind_shared)                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>399 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">ind_shared.load_state_dict(offspring.state_dict())</span>                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">400 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">401 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> ind_shared                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">402 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jaimesabal/.pyenv/versions/agilerl/lib/python3.11/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2584</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load_state_dict</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2581 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2582 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2583 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(error_msgs) &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2584 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">raise</span><span style=\"font-weight: bold; text-decoration: underline\"> </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">RuntimeError</span><span style=\"font-weight: bold; text-decoration: underline\">(</span>                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2585 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">\"Error(s) in loading state_dict for {}:\\n\\t{}\"</span><span style=\"font-weight: bold; text-decoration: underline\">.format(</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2586 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">│   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">.</span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; text-decoration: underline\">__class__</span><span style=\"font-weight: bold; text-decoration: underline\">.</span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; text-decoration: underline\">__name__</span><span style=\"font-weight: bold; text-decoration: underline\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">\"\\n\\t\"</span><span style=\"font-weight: bold; text-decoration: underline\">.join(error_msgs)</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2587 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">│   │   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Error</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span> in loading state_dict for RainbowQNetwork:\n",
       "        size mismatch for head_net.model.value_linear_layer_output.weight_mu: copying a param with shape \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span><span style=\"font-weight: bold\">])</span> from checkpoint, the shape in current model is <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">51</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span><span style=\"font-weight: bold\">])</span>.\n",
       "        size mismatch for head_net.model.value_linear_layer_output.weight_sigma: copying a param with shape \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span><span style=\"font-weight: bold\">])</span> from checkpoint, the shape in current model is <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">51</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span><span style=\"font-weight: bold\">])</span>.\n",
       "        size mismatch for head_net.model.value_linear_layer_output.bias_mu: copying a param with shape \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">])</span> from checkpoint, the shape in current model is <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">51</span><span style=\"font-weight: bold\">])</span>.\n",
       "        size mismatch for head_net.model.value_linear_layer_output.bias_sigma: copying a param with shape \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">])</span> from checkpoint, the shape in current model is <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">51</span><span style=\"font-weight: bold\">])</span>.\n",
       "        size mismatch for head_net.model.value_linear_layer_output.weight_epsilon: copying a param with shape \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span><span style=\"font-weight: bold\">])</span> from checkpoint, the shape in current model is <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">51</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span><span style=\"font-weight: bold\">])</span>.\n",
       "        size mismatch for head_net.model.value_linear_layer_output.bias_epsilon: copying a param with shape \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">])</span> from checkpoint, the shape in current model is <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">51</span><span style=\"font-weight: bold\">])</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m14\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m│   \u001b[0mdevice=device,)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0mnew_population = [agent.clone(wrap=\u001b[94mTrue\u001b[0m) \u001b[94mfor\u001b[0m agent \u001b[95min\u001b[0m agent_pop]                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m14 mutated_population = \u001b[1;4mmutations.mutation(new_population, \u001b[0m\u001b[1;4;94mTrue\u001b[0m\u001b[1;4m)\u001b[0m                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m# print([ind.mut for ind in mutated_population])\u001b[0m                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/jaimesabal/projects/AgileRL/agilerl/hpo/\u001b[0m\u001b[1;33mmutation.py\u001b[0m:\u001b[94m484\u001b[0m in \u001b[92mmutation\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m481 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[96mprint\u001b[0m(eval_offspring)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m482 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[2m# Reinitialize shared with frozen weights due to \u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m483 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[2m# potential mutation in architecture\u001b[0m                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m484 \u001b[2m│   │   │   │   │   │   \u001b[0mind_shared = \u001b[1;4;96mself\u001b[0m\u001b[1;4m.reinit_from_mutated(eval_offspring)\u001b[0m              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m485 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m486 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.accelerator \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m487 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mind_shared = \u001b[96mself\u001b[0m.to_device(ind_shared)                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/jaimesabal/projects/AgileRL/agilerl/hpo/\u001b[0m\u001b[1;33mmutation.py\u001b[0m:\u001b[94m399\u001b[0m in \u001b[92mreinit_from_mutated\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m396 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moffspring.init_dict                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m397 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m398 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mprint\u001b[0m(ind_shared)                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m399 \u001b[2m│   │   │   \u001b[0m\u001b[1;4mind_shared.load_state_dict(offspring.state_dict())\u001b[0m                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m400 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m401 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m ind_shared                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m402 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/jaimesabal/.pyenv/versions/agilerl/lib/python3.11/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m :\u001b[94m2584\u001b[0m in \u001b[92mload_state_dict\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2581 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2582 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2583 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(error_msgs) > \u001b[94m0\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2584 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mRuntimeError\u001b[0m\u001b[1;4m(\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2585 \u001b[0m\u001b[1;2;4m│   │   │   │   \u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mError(s) in loading state_dict for \u001b[0m\u001b[1;4;33m{}\u001b[0m\u001b[1;4;33m:\u001b[0m\u001b[1;4;33m\\n\u001b[0m\u001b[1;4;33m\\t\u001b[0m\u001b[1;4;33m{}\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m.format(\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2586 \u001b[0m\u001b[1;2;4m│   │   │   │   │   \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.\u001b[0m\u001b[1;4;91m__class__\u001b[0m\u001b[1;4m.\u001b[0m\u001b[1;4;91m__name__\u001b[0m\u001b[1;4m, \u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33m\\n\u001b[0m\u001b[1;4;33m\\t\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m.join(error_msgs)\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2587 \u001b[0m\u001b[1;2;4m│   │   │   │   \u001b[0m\u001b[1;4m)\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0m\u001b[1;35mError\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m in loading state_dict for RainbowQNetwork:\n",
       "        size mismatch for head_net.model.value_linear_layer_output.weight_mu: copying a param with shape \n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m from checkpoint, the shape in current model is \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m51\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m.\n",
       "        size mismatch for head_net.model.value_linear_layer_output.weight_sigma: copying a param with shape \n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m from checkpoint, the shape in current model is \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m51\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m.\n",
       "        size mismatch for head_net.model.value_linear_layer_output.bias_mu: copying a param with shape \n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m from checkpoint, the shape in current model is \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m51\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m.\n",
       "        size mismatch for head_net.model.value_linear_layer_output.bias_sigma: copying a param with shape \n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m from checkpoint, the shape in current model is \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m51\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m.\n",
       "        size mismatch for head_net.model.value_linear_layer_output.weight_epsilon: copying a param with shape \n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m from checkpoint, the shape in current model is \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m51\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m.\n",
       "        size mismatch for head_net.model.value_linear_layer_output.bias_epsilon: copying a param with shape \n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m from checkpoint, the shape in current model is \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m51\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mutations = Mutations(\n",
    "    'PPO',\n",
    "    0,\n",
    "    1,\n",
    "    0.5,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    [\"batch_size\", \"lr\", \"learn_step\"],\n",
    "    0.5,\n",
    "    device=device,)\n",
    "\n",
    "new_population = [agent.clone(wrap=True) for agent in agent_pop]\n",
    "mutated_population = mutations.mutation(new_population, True)\n",
    "# print([ind.mut for ind in mutated_population])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.model.encoder_conv_layer_1.weight',\n",
       "              tensor([[[[ 1.2011e-01, -2.3543e-01, -8.6037e-03,  3.5908e-02],\n",
       "                        [-1.8133e-02, -9.5431e-02,  1.5800e-01,  1.9248e-01],\n",
       "                        [ 1.8469e-01,  2.1109e-01, -2.7355e-03, -8.7567e-02],\n",
       "                        [-3.3392e-01, -2.5585e-01,  1.8480e-01, -1.3048e-01]],\n",
       "              \n",
       "                       [[-1.0107e-01, -7.1645e-02,  1.5379e-02, -1.8511e-03],\n",
       "                        [-1.1292e-01,  5.4702e-02, -2.5859e-01, -5.7284e-02],\n",
       "                        [-2.7937e-01,  5.6650e-02, -1.2157e-01, -3.0416e-01],\n",
       "                        [ 1.5155e-01, -2.0830e-01,  3.1269e-01, -2.5233e-02]],\n",
       "              \n",
       "                       [[-4.8939e-02,  8.9096e-02,  5.2152e-02,  1.3511e-01],\n",
       "                        [-1.1432e-05, -1.7608e-01, -2.5988e-01, -5.6473e-02],\n",
       "                        [-3.7252e-01,  1.8827e-01,  1.0439e-01,  3.1282e-01],\n",
       "                        [-1.8274e-01,  2.8279e-01,  3.5791e-01,  2.8038e-01]],\n",
       "              \n",
       "                       [[ 8.9804e-02, -5.8677e-02,  2.7004e-02, -7.0852e-02],\n",
       "                        [-2.8230e-02,  4.3004e-02, -2.4714e-02, -1.7923e-01],\n",
       "                        [ 1.6370e-01,  3.4152e-02,  7.9987e-02, -4.1203e-01],\n",
       "                        [-2.3748e-01,  1.2957e-01,  5.5091e-02,  1.3773e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1107e-01,  7.2074e-02, -3.1453e-01,  6.2464e-03],\n",
       "                        [ 5.4855e-02, -3.7036e-01, -3.3700e-01,  3.1816e-02],\n",
       "                        [ 6.3167e-01,  3.0748e-02,  2.8541e-01, -1.9061e-01],\n",
       "                        [ 9.9318e-02, -9.2246e-02,  1.2380e-01, -1.1617e-01]],\n",
       "              \n",
       "                       [[-9.1376e-02,  5.3209e-02, -3.1894e-02, -2.4113e-01],\n",
       "                        [-1.5884e-01, -7.3952e-02,  1.8294e-01, -4.6125e-03],\n",
       "                        [-1.2181e-01, -7.0672e-02, -1.2083e-01,  1.9893e-01],\n",
       "                        [ 2.9923e-01,  3.0868e-01, -1.4328e-01,  1.5708e-03]],\n",
       "              \n",
       "                       [[-8.3678e-02, -3.0063e-02,  3.7526e-02, -9.8041e-02],\n",
       "                        [-1.3222e-01, -2.1051e-03, -5.0228e-02,  2.1036e-01],\n",
       "                        [ 2.1694e-01, -1.9992e-01, -3.2453e-02,  2.0238e-01],\n",
       "                        [ 1.4789e-01, -1.1277e-01,  1.7869e-01,  7.1963e-02]],\n",
       "              \n",
       "                       [[ 9.9843e-02, -1.9505e-01,  7.7674e-02, -2.7082e-03],\n",
       "                        [ 3.9029e-02, -1.5873e-01, -2.3818e-01,  1.2174e-01],\n",
       "                        [ 1.9924e-01, -2.4638e-02, -2.7554e-03,  4.0205e-02],\n",
       "                        [ 1.0913e-01, -1.5696e-01,  3.6385e-01,  5.3011e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.9041e-02, -1.6014e-01,  9.2199e-02, -6.8461e-02],\n",
       "                        [ 1.8724e-01,  8.0130e-02,  1.5080e-01, -4.2931e-02],\n",
       "                        [ 1.6475e-01,  1.3786e-01, -1.0139e-01, -1.7455e-01],\n",
       "                        [-2.2143e-01,  5.8592e-02, -8.4089e-02,  2.0014e-01]],\n",
       "              \n",
       "                       [[ 2.4926e-01, -1.7050e-01, -3.1458e-01, -1.2425e-01],\n",
       "                        [-1.6455e-01, -4.1371e-01, -1.7275e-01,  1.1836e-01],\n",
       "                        [ 2.8510e-01, -1.0710e-01,  2.6969e-02, -2.2883e-01],\n",
       "                        [ 1.0387e-01,  7.2202e-02, -1.7806e-01,  7.9270e-02]],\n",
       "              \n",
       "                       [[ 2.7374e-01, -1.8773e-01,  2.9003e-01, -4.6847e-02],\n",
       "                        [-4.4702e-02, -1.2204e-01, -3.0922e-01,  4.8348e-02],\n",
       "                        [ 2.5525e-01, -4.3556e-02,  1.9179e-01, -5.6015e-02],\n",
       "                        [ 6.5454e-03,  4.8759e-02, -6.5314e-02, -4.1482e-01]],\n",
       "              \n",
       "                       [[-1.3044e-01, -1.2428e-01,  2.1591e-02, -1.6670e-01],\n",
       "                        [-2.0495e-01, -1.0260e-01, -1.6921e-01, -2.9909e-01],\n",
       "                        [ 2.8477e-01, -6.2529e-02, -9.2274e-03,  1.4084e-01],\n",
       "                        [ 6.0233e-02,  2.5500e-01, -1.7998e-01,  1.5873e-01]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 9.4267e-02, -1.2810e-01,  6.6303e-02,  6.0780e-02],\n",
       "                        [ 5.0643e-02, -2.0143e-01, -3.4427e-02,  1.1047e-01],\n",
       "                        [-4.0412e-02, -1.0708e-01, -2.1723e-01, -1.2981e-01],\n",
       "                        [-3.8459e-02,  8.9650e-02,  1.5192e-01, -5.7997e-02]],\n",
       "              \n",
       "                       [[-3.3811e-01, -4.0965e-03, -2.8274e-01,  9.5860e-02],\n",
       "                        [-2.5655e-01, -3.1848e-02,  1.9202e-01, -4.4868e-01],\n",
       "                        [ 2.9863e-01,  2.0459e-01,  1.7338e-02,  1.9129e-01],\n",
       "                        [-2.4265e-01, -8.4460e-02,  2.4479e-01, -9.9731e-02]],\n",
       "              \n",
       "                       [[ 1.6892e-01, -1.4194e-02, -5.9123e-03,  1.3460e-01],\n",
       "                        [-1.7432e-01, -2.9581e-01,  1.5870e-01, -1.2473e-01],\n",
       "                        [ 1.1753e-01,  6.1209e-02,  1.3639e-01, -8.4989e-02],\n",
       "                        [-9.9626e-02, -9.3963e-02, -4.9895e-02,  3.3045e-01]],\n",
       "              \n",
       "                       [[-2.3058e-01, -3.5004e-01, -9.5699e-02, -3.8078e-01],\n",
       "                        [-1.2707e-01,  2.0823e-01, -1.3419e-01,  1.4048e-01],\n",
       "                        [ 1.2921e-01, -1.1034e-01,  1.1983e-01,  2.0239e-01],\n",
       "                        [-9.9487e-03, -2.5035e-01, -1.4390e-01, -1.0286e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.7585e-01, -2.6824e-01,  1.7037e-01, -1.9229e-01],\n",
       "                        [ 1.1353e-01,  2.3247e-01, -2.2408e-01,  1.7776e-01],\n",
       "                        [ 4.1535e-01,  1.5175e-01, -9.0450e-02, -3.5494e-01],\n",
       "                        [-3.0910e-02,  3.3555e-01, -1.0437e-01, -9.9014e-03]],\n",
       "              \n",
       "                       [[ 1.1647e-01, -3.5236e-02, -2.1880e-02,  2.3412e-01],\n",
       "                        [-2.0037e-01,  1.4811e-01, -1.6062e-01,  1.0957e-01],\n",
       "                        [-1.4924e-01,  1.3139e-02,  6.6558e-02, -2.4374e-01],\n",
       "                        [ 2.1279e-01,  2.1419e-02, -1.1269e-02,  5.7992e-02]],\n",
       "              \n",
       "                       [[-3.0696e-01,  7.9110e-03, -1.8179e-01,  2.5534e-01],\n",
       "                        [-9.8161e-02, -2.1899e-01,  2.2034e-01, -2.7321e-01],\n",
       "                        [ 1.8026e-01,  5.3833e-02, -5.6990e-02, -1.7128e-01],\n",
       "                        [ 4.6562e-03, -9.3539e-03, -3.9721e-01, -2.3138e-02]],\n",
       "              \n",
       "                       [[ 2.7893e-01, -1.1527e-01, -2.8283e-01,  7.3017e-02],\n",
       "                        [-1.2532e-02, -5.8219e-02,  2.2593e-01,  1.0784e-01],\n",
       "                        [-1.0642e-01,  6.9503e-04,  9.0144e-02, -1.2831e-01],\n",
       "                        [ 4.7002e-02, -1.4687e-01,  5.4863e-02, -1.3256e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.3627e-02,  2.9010e-01,  1.4430e-01, -6.7833e-02],\n",
       "                        [ 1.0944e-01,  2.6750e-01,  1.0960e-02,  2.8953e-01],\n",
       "                        [ 1.0643e-02, -1.7182e-01,  1.2802e-01, -6.9147e-02],\n",
       "                        [ 9.4883e-05, -1.0955e-01, -1.4471e-01, -1.8639e-01]],\n",
       "              \n",
       "                       [[-2.1994e-01, -2.3579e-01,  5.5414e-02, -2.7230e-02],\n",
       "                        [ 1.8455e-02, -5.2442e-02,  1.1061e-01,  2.6582e-02],\n",
       "                        [ 2.9508e-02,  8.6890e-02, -2.8366e-03,  2.5296e-01],\n",
       "                        [-1.1167e-01,  5.3331e-02,  9.8459e-03,  7.4608e-02]],\n",
       "              \n",
       "                       [[ 5.3238e-03, -8.3735e-02, -6.3682e-02,  2.2960e-01],\n",
       "                        [ 1.5238e-01,  7.7715e-02, -3.1816e-01, -5.3728e-02],\n",
       "                        [ 1.8861e-01,  7.3357e-02,  1.4209e-01,  3.0953e-01],\n",
       "                        [-5.0695e-01, -1.8679e-01,  1.1660e-01, -1.8949e-02]],\n",
       "              \n",
       "                       [[ 3.7341e-01,  2.7684e-02, -4.2918e-01, -1.2789e-01],\n",
       "                        [-5.9681e-02,  3.9809e-02,  1.2472e-01,  3.6296e-02],\n",
       "                        [-4.1718e-03,  3.6855e-01, -1.7901e-01,  1.7528e-01],\n",
       "                        [ 3.1612e-01,  2.1096e-01,  1.8753e-02,  1.4458e-01]]]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_conv_layer_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_layer_norm_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_layer_norm_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_layer_norm_1.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_layer_norm_1.running_var',\n",
       "              tensor([0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_layer_norm_1.num_batches_tracked',\n",
       "              tensor(2, device='cuda:0')),\n",
       "             ('encoder.model.encoder_linear_output.weight',\n",
       "              tensor([[-2.6982e-03,  8.8416e-04, -1.6105e-03,  ..., -4.2469e-04,\n",
       "                        3.0841e-03,  8.7087e-04],\n",
       "                      [-1.0762e-03,  2.9336e-03,  1.1011e-03,  ...,  1.1705e-03,\n",
       "                        1.9205e-03,  1.7049e-03],\n",
       "                      [ 2.1716e-03, -3.7796e-04, -9.7582e-04,  ..., -1.4043e-03,\n",
       "                       -8.7901e-04,  2.1939e-04],\n",
       "                      ...,\n",
       "                      [ 2.3385e-03, -6.7821e-04, -6.1678e-04,  ...,  2.5169e-03,\n",
       "                        6.2754e-04, -2.3831e-03],\n",
       "                      [ 2.5204e-03, -2.0914e-04,  5.1959e-04,  ..., -2.9482e-03,\n",
       "                        2.4382e-03,  2.7641e-04],\n",
       "                      [ 3.9437e-04, -6.7143e-05, -1.6697e-03,  ..., -1.0852e-03,\n",
       "                       -2.4008e-04, -2.8168e-03]], device='cuda:0')),\n",
       "             ('encoder.model.encoder_linear_output.bias',\n",
       "              tensor([ 0.0022,  0.0020, -0.0021,  0.0013,  0.0014, -0.0027, -0.0001, -0.0027,\n",
       "                       0.0010,  0.0007, -0.0026,  0.0004,  0.0025, -0.0002,  0.0019,  0.0008,\n",
       "                      -0.0021,  0.0005, -0.0020,  0.0019, -0.0011, -0.0021, -0.0022, -0.0023,\n",
       "                       0.0015, -0.0022, -0.0029, -0.0010, -0.0029, -0.0025,  0.0016, -0.0015],\n",
       "                     device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_linear_layer_1.weight',\n",
       "              tensor([[ 1.9515e-01, -1.6597e-01, -4.0421e-02,  1.7122e-01, -9.1142e-03,\n",
       "                        9.4596e-02, -5.9549e-01, -1.1075e-01,  1.4865e-01, -2.4306e-01,\n",
       "                       -6.8971e-02,  1.3106e-01, -4.1923e-01,  1.8785e-01,  1.1577e-02,\n",
       "                       -3.8778e-02,  4.0489e-01, -7.6712e-01,  1.9766e-01, -3.0815e-01,\n",
       "                        1.7862e-01, -1.4948e-01,  2.6292e-02, -1.9560e-02,  1.4881e-01,\n",
       "                       -2.4611e-01,  5.8229e-02,  1.7403e-01, -1.1102e-01,  3.6546e-01,\n",
       "                       -2.3131e-02, -8.4586e-02],\n",
       "                      [ 3.2324e-01, -2.9664e-01,  2.7525e-01,  1.4115e-01, -4.2086e-01,\n",
       "                       -1.6181e-01,  2.6927e-01, -1.2423e-01, -6.1818e-02,  1.2901e-01,\n",
       "                       -2.3235e-01,  1.4302e-01, -4.1033e-01,  4.9666e-02,  3.9880e-01,\n",
       "                       -1.6450e-01,  1.0063e-01,  3.0905e-01, -2.0553e-01,  2.8991e-01,\n",
       "                        3.7790e-01, -7.9027e-03, -2.4783e-01, -7.5432e-02,  3.5815e-01,\n",
       "                       -5.6521e-02,  1.0957e-01, -3.0360e-01,  3.7955e-01,  3.5437e-01,\n",
       "                       -1.4347e-01,  3.1266e-02],\n",
       "                      [ 1.4282e-02,  5.1707e-01,  1.2399e-01,  1.7680e-01, -1.0161e-01,\n",
       "                       -3.5483e-01, -4.9666e-02,  1.5923e-01,  1.9858e-01,  4.3562e-01,\n",
       "                        1.2665e-01, -4.2984e-02,  2.7459e-01, -1.0000e-01,  3.9929e-01,\n",
       "                       -1.1605e-01,  1.7680e-02, -5.9203e-01, -1.6694e-01,  3.6475e-02,\n",
       "                        3.3514e-02, -8.3827e-02, -2.2107e-01,  4.2061e-01,  3.7079e-01,\n",
       "                        2.8971e-01, -1.7712e-01,  1.3626e-01,  2.0414e-01, -2.3689e-01,\n",
       "                        9.3902e-03,  1.8214e-01],\n",
       "                      [ 2.8718e-01, -1.9452e-01, -2.7487e-01,  7.0442e-02, -3.4681e-01,\n",
       "                        2.0146e-01, -1.9128e-01,  3.4341e-01, -2.4071e-01, -6.5058e-02,\n",
       "                       -2.4306e-01,  8.9122e-02, -1.4885e-01, -1.0703e-01,  2.8016e-02,\n",
       "                        9.9423e-02, -3.6061e-01, -1.9426e-01,  1.0413e-02, -2.6461e-01,\n",
       "                        1.7129e-01,  1.1770e-01,  4.5449e-01,  9.3120e-02, -1.2538e-01,\n",
       "                        5.1206e-01, -1.1480e-01, -3.1633e-01,  4.1293e-01, -3.6830e-01,\n",
       "                        1.8147e-01, -2.8938e-01],\n",
       "                      [ 4.1197e-01,  3.0449e-01,  1.3845e-01, -4.6537e-01, -3.9082e-01,\n",
       "                        2.3002e-01,  1.5786e-02, -1.2519e-01,  5.5482e-01, -2.7075e-01,\n",
       "                        1.3094e-02, -7.2170e-02,  1.8341e-03, -8.5223e-02,  7.7159e-03,\n",
       "                       -3.0173e-02,  5.6188e-02,  2.7987e-01,  2.2084e-01, -4.7378e-01,\n",
       "                       -3.0353e-02, -1.4822e-01, -3.5095e-01,  3.5921e-01, -3.3229e-02,\n",
       "                       -2.2243e-01,  2.0088e-01, -2.0622e-01,  1.0033e-02, -3.3748e-01,\n",
       "                        6.4750e-02, -6.3620e-02],\n",
       "                      [-4.1156e-01, -1.9112e-01,  2.2870e-01,  2.1598e-01, -1.0535e-02,\n",
       "                        7.6662e-02, -8.7347e-02, -1.1143e-01,  2.7922e-01, -9.3367e-02,\n",
       "                       -2.9914e-01, -3.0783e-01,  9.2305e-02, -1.9867e-01,  5.9002e-01,\n",
       "                        1.2712e-02, -2.0757e-01,  2.5057e-03, -2.4672e-01, -3.2041e-02,\n",
       "                        9.1949e-02, -1.0199e-01,  3.9901e-01,  1.0763e-01,  1.8202e-02,\n",
       "                       -1.1405e-01,  4.4327e-01,  2.9572e-02, -3.8114e-01, -2.3501e-01,\n",
       "                       -3.9205e-01, -3.7730e-01],\n",
       "                      [-3.6891e-01,  1.5913e-01, -9.2283e-02, -3.8851e-03, -1.0125e-01,\n",
       "                       -5.3490e-01,  2.4234e-01, -3.3741e-01,  5.4065e-02, -3.2773e-01,\n",
       "                        3.4789e-01,  1.2058e-01,  2.5072e-02,  3.1860e-01,  1.5936e-01,\n",
       "                        3.1721e-01, -5.2090e-02, -1.1264e-01,  4.8201e-02, -2.8575e-01,\n",
       "                        2.5540e-01,  1.0574e-02, -2.5219e-01, -2.2614e-01, -3.4891e-01,\n",
       "                        3.8936e-01,  2.7896e-01,  7.2532e-03,  2.4737e-01,  1.4497e-01,\n",
       "                        5.9661e-02, -4.1305e-01],\n",
       "                      [ 2.0571e-02, -1.9100e-01, -7.2230e-02,  2.2537e-01, -3.2966e-01,\n",
       "                       -1.4746e-01, -1.0134e-01,  7.5936e-02,  2.3046e-01,  7.3356e-02,\n",
       "                        6.1123e-03,  3.4040e-01,  6.1425e-01,  2.4324e-01, -2.7914e-01,\n",
       "                        2.5148e-01, -1.8915e-01, -3.7871e-03,  2.1524e-02,  2.0411e-01,\n",
       "                       -9.8206e-02, -1.1627e-01, -7.1841e-02, -4.3628e-02,  4.3795e-01,\n",
       "                       -3.3081e-01, -2.2450e-01, -3.1202e-01, -2.5061e-01,  7.6602e-02,\n",
       "                        2.0830e-01, -5.8258e-01],\n",
       "                      [-3.2612e-02,  2.7017e-01,  1.6972e-01, -3.4581e-02,  1.2357e-01,\n",
       "                       -3.4653e-01, -4.9093e-01, -1.4127e-02, -2.1082e-01,  9.3412e-02,\n",
       "                       -3.4421e-01, -9.3607e-02,  7.2965e-02,  1.1887e-01, -1.4554e-01,\n",
       "                        2.3706e-01,  1.5074e-01,  3.0441e-01,  5.4755e-01,  1.0431e-01,\n",
       "                       -1.1015e-01, -1.7963e-01,  9.4604e-02,  3.5877e-01, -7.2780e-02,\n",
       "                        2.5650e-01, -4.0245e-02, -3.4036e-01,  1.0604e-01,  2.3778e-01,\n",
       "                       -6.1080e-01, -6.7676e-02],\n",
       "                      [ 1.6799e-02,  5.2523e-01, -4.1892e-01, -5.2771e-02, -4.3528e-02,\n",
       "                       -1.0307e-01,  3.2874e-02, -1.5463e-01, -3.8645e-01, -2.2206e-01,\n",
       "                       -5.3243e-01, -1.3849e-01, -1.8205e-01,  4.3128e-01,  3.4016e-01,\n",
       "                       -7.0650e-02, -1.3617e-01,  9.7414e-02, -2.6537e-01, -1.7763e-01,\n",
       "                       -5.5483e-01, -6.7608e-02,  2.5266e-02, -6.5369e-02,  3.0631e-01,\n",
       "                       -1.2370e-01, -1.9174e-02,  2.1669e-04, -1.3437e-01,  1.0330e-01,\n",
       "                        2.8397e-01, -2.6885e-02],\n",
       "                      [-3.4560e-01,  2.6560e-02, -1.6175e-01, -8.8353e-02,  1.1566e-01,\n",
       "                       -1.2812e-01, -4.0081e-01,  1.0846e-01,  5.4816e-01, -1.0317e-01,\n",
       "                        1.4411e-01,  9.5444e-02, -4.0483e-01,  2.5015e-01, -1.8348e-01,\n",
       "                        2.5964e-01, -1.9624e-01,  1.8399e-01, -4.4645e-01,  2.8309e-01,\n",
       "                        7.2924e-02,  1.9609e-01,  2.0325e-01,  4.3294e-01,  6.3699e-02,\n",
       "                       -2.9116e-02,  1.8416e-01, -2.4917e-01,  1.2805e-01,  1.0263e-01,\n",
       "                        2.8359e-01,  3.7755e-01],\n",
       "                      [ 3.2088e-02, -1.2251e-01,  4.4453e-01,  2.9743e-02,  1.7537e-02,\n",
       "                        1.2132e-01,  5.3092e-01,  1.8737e-01,  7.4483e-02, -8.8563e-02,\n",
       "                        1.9277e-02, -4.5306e-01, -1.9043e-01,  4.7862e-01, -4.0429e-01,\n",
       "                        8.4665e-02, -1.6240e-01, -4.3579e-02,  1.1788e-01, -6.1571e-02,\n",
       "                       -7.1534e-02, -2.1737e-01,  2.7024e-01,  3.7017e-01,  3.5456e-01,\n",
       "                        1.8654e-01,  1.3651e-03,  4.6888e-01,  1.7160e-01,  1.6879e-01,\n",
       "                        5.2714e-02, -2.2139e-01],\n",
       "                      [-3.3176e-01, -6.6215e-02,  1.3895e-01,  8.5469e-02, -2.2199e-01,\n",
       "                        5.4277e-01,  1.6542e-01,  2.5174e-02, -1.1926e-01,  1.8397e-01,\n",
       "                       -3.6480e-01,  1.8529e-01,  3.8378e-01,  1.5864e-01, -2.2765e-02,\n",
       "                        7.1941e-02,  2.1980e-01, -2.8177e-01,  6.8229e-02, -4.4710e-02,\n",
       "                       -2.7068e-01,  4.9051e-01, -2.2540e-01,  3.0151e-01, -1.9722e-01,\n",
       "                        6.2168e-02,  5.2798e-01, -1.3716e-01,  1.6401e-01,  3.0909e-01,\n",
       "                        1.2935e-01,  1.5487e-01],\n",
       "                      [-1.0008e-01, -1.3340e-01,  1.7132e-01,  1.7500e-02, -3.2413e-01,\n",
       "                        1.3333e-01, -3.9212e-01,  1.3218e-01, -2.8467e-01, -5.0200e-01,\n",
       "                        2.3386e-01,  4.8619e-02,  6.8238e-02,  5.1684e-01,  2.7227e-01,\n",
       "                       -2.9752e-01, -2.7249e-01,  8.9261e-02,  1.6182e-01,  3.9764e-01,\n",
       "                        2.4940e-02,  2.6525e-01, -3.3338e-01,  1.7955e-01, -1.3938e-01,\n",
       "                       -6.1873e-02, -2.2041e-01,  3.6102e-01, -1.9394e-02, -3.1389e-01,\n",
       "                       -1.9317e-01,  6.8371e-02],\n",
       "                      [ 1.2645e-01,  2.4378e-02, -2.2781e-01, -4.9132e-01, -2.6346e-02,\n",
       "                       -2.8866e-01,  2.4808e-01, -4.6752e-01,  4.5194e-02,  1.6585e-01,\n",
       "                       -2.0504e-01,  4.9931e-01,  9.7349e-02,  9.9132e-02, -1.3775e-01,\n",
       "                       -4.6768e-01, -2.8464e-01, -1.8994e-01,  2.1707e-01,  1.4220e-01,\n",
       "                        2.4917e-01,  2.4619e-01,  4.8330e-01,  2.8794e-01, -2.3489e-03,\n",
       "                       -1.1363e-01,  1.3105e-01,  2.4256e-01, -1.0167e-01,  1.4712e-03,\n",
       "                       -1.5868e-01,  3.7644e-03],\n",
       "                      [ 3.5874e-01, -4.5782e-02, -3.1940e-01,  2.3702e-01, -6.5051e-02,\n",
       "                       -1.5597e-01, -9.5117e-02, -1.8051e-01,  4.4031e-01,  2.9589e-02,\n",
       "                       -1.7037e-01, -3.5579e-01, -2.1074e-01, -2.3445e-01, -1.9725e-01,\n",
       "                       -1.0646e-01,  9.6421e-02,  4.1068e-03,  4.9895e-02,  4.7207e-01,\n",
       "                       -4.1591e-01,  5.0237e-01, -2.6876e-01, -6.0585e-02, -1.0551e-01,\n",
       "                        3.3990e-01,  1.3580e-01,  2.9199e-01, -1.5700e-02, -1.3792e-02,\n",
       "                       -2.4712e-02, -4.1654e-01]], device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_linear_layer_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_layer_norm_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "                     device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_layer_norm_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_linear_layer_output.weight',\n",
       "              tensor([[ 0.0184, -0.0454, -0.0361,  0.0252,  0.0644,  0.0251,  0.0382,  0.0006,\n",
       "                        0.0033,  0.0088,  0.0326, -0.0458,  0.0097, -0.0562, -0.0317,  0.0438],\n",
       "                      [-0.0247,  0.0385, -0.0544,  0.0617, -0.0151, -0.0207,  0.0282, -0.0078,\n",
       "                        0.0173, -0.0472, -0.0198,  0.0494,  0.0123, -0.0238, -0.0562, -0.0199],\n",
       "                      [-0.0498,  0.0315,  0.0396, -0.0606,  0.0587, -0.0039,  0.0120,  0.0128,\n",
       "                       -0.0122, -0.0247, -0.0627, -0.0113, -0.0044, -0.0469, -0.0209, -0.0073],\n",
       "                      [-0.0083, -0.0331, -0.0049,  0.0142,  0.0253,  0.0091, -0.0356,  0.0341,\n",
       "                        0.0961, -0.0304, -0.0008, -0.0007, -0.0253, -0.0222,  0.0439, -0.0473]],\n",
       "                     device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_linear_layer_output.bias',\n",
       "              tensor([0., 0., 0., 0.], device='cuda:0'))])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutated_population[0].actor.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = mutated_population[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.model.encoder_conv_layer_1.weight',\n",
       "              tensor([[[[ 1.2011e-01, -2.3543e-01, -8.6037e-03,  3.5908e-02],\n",
       "                        [-1.8133e-02, -9.5431e-02,  1.5800e-01,  1.9248e-01],\n",
       "                        [ 1.8469e-01,  2.1109e-01, -2.7355e-03, -8.7567e-02],\n",
       "                        [-3.3392e-01, -2.5585e-01,  1.8480e-01, -1.3048e-01]],\n",
       "              \n",
       "                       [[-1.0107e-01, -7.1645e-02,  1.5379e-02, -1.8511e-03],\n",
       "                        [-1.1292e-01,  5.4702e-02, -2.5859e-01, -5.7284e-02],\n",
       "                        [-2.7937e-01,  5.6650e-02, -1.2157e-01, -3.0416e-01],\n",
       "                        [ 1.5155e-01, -2.0830e-01,  3.1269e-01, -2.5233e-02]],\n",
       "              \n",
       "                       [[-4.8939e-02,  8.9096e-02,  5.2152e-02,  1.3511e-01],\n",
       "                        [-1.1432e-05, -1.7608e-01, -2.5988e-01, -5.6473e-02],\n",
       "                        [-3.7252e-01,  1.8827e-01,  1.0439e-01,  3.1282e-01],\n",
       "                        [-1.8274e-01,  2.8279e-01,  3.5791e-01,  2.8038e-01]],\n",
       "              \n",
       "                       [[ 8.9804e-02, -5.8677e-02,  2.7004e-02, -7.0852e-02],\n",
       "                        [-2.8230e-02,  4.3004e-02, -2.4714e-02, -1.7923e-01],\n",
       "                        [ 1.6370e-01,  3.4152e-02,  7.9987e-02, -4.1203e-01],\n",
       "                        [-2.3748e-01,  1.2957e-01,  5.5091e-02,  1.3773e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1107e-01,  7.2074e-02, -3.1453e-01,  6.2464e-03],\n",
       "                        [ 5.4855e-02, -3.7036e-01, -3.3700e-01,  3.1816e-02],\n",
       "                        [ 6.3167e-01,  3.0748e-02,  2.8541e-01, -1.9061e-01],\n",
       "                        [ 9.9318e-02, -9.2246e-02,  1.2380e-01, -1.1617e-01]],\n",
       "              \n",
       "                       [[-9.1376e-02,  5.3209e-02, -3.1894e-02, -2.4113e-01],\n",
       "                        [-1.5884e-01, -7.3952e-02,  1.8294e-01, -4.6125e-03],\n",
       "                        [-1.2181e-01, -7.0672e-02, -1.2083e-01,  1.9893e-01],\n",
       "                        [ 2.9923e-01,  3.0868e-01, -1.4328e-01,  1.5708e-03]],\n",
       "              \n",
       "                       [[-8.3678e-02, -3.0063e-02,  3.7526e-02, -9.8041e-02],\n",
       "                        [-1.3222e-01, -2.1051e-03, -5.0228e-02,  2.1036e-01],\n",
       "                        [ 2.1694e-01, -1.9992e-01, -3.2453e-02,  2.0238e-01],\n",
       "                        [ 1.4789e-01, -1.1277e-01,  1.7869e-01,  7.1963e-02]],\n",
       "              \n",
       "                       [[ 9.9843e-02, -1.9505e-01,  7.7674e-02, -2.7082e-03],\n",
       "                        [ 3.9029e-02, -1.5873e-01, -2.3818e-01,  1.2174e-01],\n",
       "                        [ 1.9924e-01, -2.4638e-02, -2.7554e-03,  4.0205e-02],\n",
       "                        [ 1.0913e-01, -1.5696e-01,  3.6385e-01,  5.3011e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.9041e-02, -1.6014e-01,  9.2199e-02, -6.8461e-02],\n",
       "                        [ 1.8724e-01,  8.0130e-02,  1.5080e-01, -4.2931e-02],\n",
       "                        [ 1.6475e-01,  1.3786e-01, -1.0139e-01, -1.7455e-01],\n",
       "                        [-2.2143e-01,  5.8592e-02, -8.4089e-02,  2.0014e-01]],\n",
       "              \n",
       "                       [[ 2.4926e-01, -1.7050e-01, -3.1458e-01, -1.2425e-01],\n",
       "                        [-1.6455e-01, -4.1371e-01, -1.7275e-01,  1.1836e-01],\n",
       "                        [ 2.8510e-01, -1.0710e-01,  2.6969e-02, -2.2883e-01],\n",
       "                        [ 1.0387e-01,  7.2202e-02, -1.7806e-01,  7.9270e-02]],\n",
       "              \n",
       "                       [[ 2.7374e-01, -1.8773e-01,  2.9003e-01, -4.6847e-02],\n",
       "                        [-4.4702e-02, -1.2204e-01, -3.0922e-01,  4.8348e-02],\n",
       "                        [ 2.5525e-01, -4.3556e-02,  1.9179e-01, -5.6015e-02],\n",
       "                        [ 6.5454e-03,  4.8759e-02, -6.5314e-02, -4.1482e-01]],\n",
       "              \n",
       "                       [[-1.3044e-01, -1.2428e-01,  2.1591e-02, -1.6670e-01],\n",
       "                        [-2.0495e-01, -1.0260e-01, -1.6921e-01, -2.9909e-01],\n",
       "                        [ 2.8477e-01, -6.2529e-02, -9.2274e-03,  1.4084e-01],\n",
       "                        [ 6.0233e-02,  2.5500e-01, -1.7998e-01,  1.5873e-01]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 9.4267e-02, -1.2810e-01,  6.6303e-02,  6.0780e-02],\n",
       "                        [ 5.0643e-02, -2.0143e-01, -3.4427e-02,  1.1047e-01],\n",
       "                        [-4.0412e-02, -1.0708e-01, -2.1723e-01, -1.2981e-01],\n",
       "                        [-3.8459e-02,  8.9650e-02,  1.5192e-01, -5.7997e-02]],\n",
       "              \n",
       "                       [[-3.3811e-01, -4.0965e-03, -2.8274e-01,  9.5860e-02],\n",
       "                        [-2.5655e-01, -3.1848e-02,  1.9202e-01, -4.4868e-01],\n",
       "                        [ 2.9863e-01,  2.0459e-01,  1.7338e-02,  1.9129e-01],\n",
       "                        [-2.4265e-01, -8.4460e-02,  2.4479e-01, -9.9731e-02]],\n",
       "              \n",
       "                       [[ 1.6892e-01, -1.4194e-02, -5.9123e-03,  1.3460e-01],\n",
       "                        [-1.7432e-01, -2.9581e-01,  1.5870e-01, -1.2473e-01],\n",
       "                        [ 1.1753e-01,  6.1209e-02,  1.3639e-01, -8.4989e-02],\n",
       "                        [-9.9626e-02, -9.3963e-02, -4.9895e-02,  3.3045e-01]],\n",
       "              \n",
       "                       [[-2.3058e-01, -3.5004e-01, -9.5699e-02, -3.8078e-01],\n",
       "                        [-1.2707e-01,  2.0823e-01, -1.3419e-01,  1.4048e-01],\n",
       "                        [ 1.2921e-01, -1.1034e-01,  1.1983e-01,  2.0239e-01],\n",
       "                        [-9.9487e-03, -2.5035e-01, -1.4390e-01, -1.0286e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.7585e-01, -2.6824e-01,  1.7037e-01, -1.9229e-01],\n",
       "                        [ 1.1353e-01,  2.3247e-01, -2.2408e-01,  1.7776e-01],\n",
       "                        [ 4.1535e-01,  1.5175e-01, -9.0450e-02, -3.5494e-01],\n",
       "                        [-3.0910e-02,  3.3555e-01, -1.0437e-01, -9.9014e-03]],\n",
       "              \n",
       "                       [[ 1.1647e-01, -3.5236e-02, -2.1880e-02,  2.3412e-01],\n",
       "                        [-2.0037e-01,  1.4811e-01, -1.6062e-01,  1.0957e-01],\n",
       "                        [-1.4924e-01,  1.3139e-02,  6.6558e-02, -2.4374e-01],\n",
       "                        [ 2.1279e-01,  2.1419e-02, -1.1269e-02,  5.7992e-02]],\n",
       "              \n",
       "                       [[-3.0696e-01,  7.9110e-03, -1.8179e-01,  2.5534e-01],\n",
       "                        [-9.8161e-02, -2.1899e-01,  2.2034e-01, -2.7321e-01],\n",
       "                        [ 1.8026e-01,  5.3833e-02, -5.6990e-02, -1.7128e-01],\n",
       "                        [ 4.6562e-03, -9.3539e-03, -3.9721e-01, -2.3138e-02]],\n",
       "              \n",
       "                       [[ 2.7893e-01, -1.1527e-01, -2.8283e-01,  7.3017e-02],\n",
       "                        [-1.2532e-02, -5.8219e-02,  2.2593e-01,  1.0784e-01],\n",
       "                        [-1.0642e-01,  6.9503e-04,  9.0144e-02, -1.2831e-01],\n",
       "                        [ 4.7002e-02, -1.4687e-01,  5.4863e-02, -1.3256e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.3627e-02,  2.9010e-01,  1.4430e-01, -6.7833e-02],\n",
       "                        [ 1.0944e-01,  2.6750e-01,  1.0960e-02,  2.8953e-01],\n",
       "                        [ 1.0643e-02, -1.7182e-01,  1.2802e-01, -6.9147e-02],\n",
       "                        [ 9.4883e-05, -1.0955e-01, -1.4471e-01, -1.8639e-01]],\n",
       "              \n",
       "                       [[-2.1994e-01, -2.3579e-01,  5.5414e-02, -2.7230e-02],\n",
       "                        [ 1.8455e-02, -5.2442e-02,  1.1061e-01,  2.6582e-02],\n",
       "                        [ 2.9508e-02,  8.6890e-02, -2.8366e-03,  2.5296e-01],\n",
       "                        [-1.1167e-01,  5.3331e-02,  9.8459e-03,  7.4608e-02]],\n",
       "              \n",
       "                       [[ 5.3238e-03, -8.3735e-02, -6.3682e-02,  2.2960e-01],\n",
       "                        [ 1.5238e-01,  7.7715e-02, -3.1816e-01, -5.3728e-02],\n",
       "                        [ 1.8861e-01,  7.3357e-02,  1.4209e-01,  3.0953e-01],\n",
       "                        [-5.0695e-01, -1.8679e-01,  1.1660e-01, -1.8949e-02]],\n",
       "              \n",
       "                       [[ 3.7341e-01,  2.7684e-02, -4.2918e-01, -1.2789e-01],\n",
       "                        [-5.9681e-02,  3.9809e-02,  1.2472e-01,  3.6296e-02],\n",
       "                        [-4.1718e-03,  3.6855e-01, -1.7901e-01,  1.7528e-01],\n",
       "                        [ 3.1612e-01,  2.1096e-01,  1.8753e-02,  1.4458e-01]]]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_conv_layer_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_layer_norm_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_layer_norm_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_layer_norm_1.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_layer_norm_1.running_var',\n",
       "              tensor([0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_layer_norm_1.num_batches_tracked',\n",
       "              tensor(2, device='cuda:0')),\n",
       "             ('encoder.model.encoder_linear_output.weight',\n",
       "              tensor([[-2.6982e-03,  8.8416e-04, -1.6105e-03,  ..., -4.2469e-04,\n",
       "                        3.0841e-03,  8.7087e-04],\n",
       "                      [-1.0762e-03,  2.9336e-03,  1.1011e-03,  ...,  1.1705e-03,\n",
       "                        1.9205e-03,  1.7049e-03],\n",
       "                      [ 2.1716e-03, -3.7796e-04, -9.7582e-04,  ..., -1.4043e-03,\n",
       "                       -8.7901e-04,  2.1939e-04],\n",
       "                      ...,\n",
       "                      [ 2.3385e-03, -6.7821e-04, -6.1678e-04,  ...,  2.5169e-03,\n",
       "                        6.2754e-04, -2.3831e-03],\n",
       "                      [ 2.5204e-03, -2.0914e-04,  5.1959e-04,  ..., -2.9482e-03,\n",
       "                        2.4382e-03,  2.7641e-04],\n",
       "                      [ 3.9437e-04, -6.7143e-05, -1.6697e-03,  ..., -1.0852e-03,\n",
       "                       -2.4008e-04, -2.8168e-03]], device='cuda:0')),\n",
       "             ('encoder.model.encoder_linear_output.bias',\n",
       "              tensor([ 0.0022,  0.0020, -0.0021,  0.0013,  0.0014, -0.0027, -0.0001, -0.0027,\n",
       "                       0.0010,  0.0007, -0.0026,  0.0004,  0.0025, -0.0002,  0.0019,  0.0008,\n",
       "                      -0.0021,  0.0005, -0.0020,  0.0019, -0.0011, -0.0021, -0.0022, -0.0023,\n",
       "                       0.0015, -0.0022, -0.0029, -0.0010, -0.0029, -0.0025,  0.0016, -0.0015],\n",
       "                     device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_linear_layer_1.weight',\n",
       "              tensor([[ 1.9515e-01, -1.6597e-01, -4.0421e-02,  1.7122e-01, -9.1142e-03,\n",
       "                        9.4596e-02, -5.9549e-01, -1.1075e-01,  1.4865e-01, -2.4306e-01,\n",
       "                       -6.8971e-02,  1.3106e-01, -4.1923e-01,  1.8785e-01,  1.1577e-02,\n",
       "                       -3.8778e-02,  4.0489e-01, -7.6712e-01,  1.9766e-01, -3.0815e-01,\n",
       "                        1.7862e-01, -1.4948e-01,  2.6292e-02, -1.9560e-02,  1.4881e-01,\n",
       "                       -2.4611e-01,  5.8229e-02,  1.7403e-01, -1.1102e-01,  3.6546e-01,\n",
       "                       -2.3131e-02, -8.4586e-02],\n",
       "                      [ 3.2324e-01, -2.9664e-01,  2.7525e-01,  1.4115e-01, -4.2086e-01,\n",
       "                       -1.6181e-01,  2.6927e-01, -1.2423e-01, -6.1818e-02,  1.2901e-01,\n",
       "                       -2.3235e-01,  1.4302e-01, -4.1033e-01,  4.9666e-02,  3.9880e-01,\n",
       "                       -1.6450e-01,  1.0063e-01,  3.0905e-01, -2.0553e-01,  2.8991e-01,\n",
       "                        3.7790e-01, -7.9027e-03, -2.4783e-01, -7.5432e-02,  3.5815e-01,\n",
       "                       -5.6521e-02,  1.0957e-01, -3.0360e-01,  3.7955e-01,  3.5437e-01,\n",
       "                       -1.4347e-01,  3.1266e-02],\n",
       "                      [ 1.4282e-02,  5.1707e-01,  1.2399e-01,  1.7680e-01, -1.0161e-01,\n",
       "                       -3.5483e-01, -4.9666e-02,  1.5923e-01,  1.9858e-01,  4.3562e-01,\n",
       "                        1.2665e-01, -4.2984e-02,  2.7459e-01, -1.0000e-01,  3.9929e-01,\n",
       "                       -1.1605e-01,  1.7680e-02, -5.9203e-01, -1.6694e-01,  3.6475e-02,\n",
       "                        3.3514e-02, -8.3827e-02, -2.2107e-01,  4.2061e-01,  3.7079e-01,\n",
       "                        2.8971e-01, -1.7712e-01,  1.3626e-01,  2.0414e-01, -2.3689e-01,\n",
       "                        9.3902e-03,  1.8214e-01],\n",
       "                      [ 2.8718e-01, -1.9452e-01, -2.7487e-01,  7.0442e-02, -3.4681e-01,\n",
       "                        2.0146e-01, -1.9128e-01,  3.4341e-01, -2.4071e-01, -6.5058e-02,\n",
       "                       -2.4306e-01,  8.9122e-02, -1.4885e-01, -1.0703e-01,  2.8016e-02,\n",
       "                        9.9423e-02, -3.6061e-01, -1.9426e-01,  1.0413e-02, -2.6461e-01,\n",
       "                        1.7129e-01,  1.1770e-01,  4.5449e-01,  9.3120e-02, -1.2538e-01,\n",
       "                        5.1206e-01, -1.1480e-01, -3.1633e-01,  4.1293e-01, -3.6830e-01,\n",
       "                        1.8147e-01, -2.8938e-01],\n",
       "                      [ 4.1197e-01,  3.0449e-01,  1.3845e-01, -4.6537e-01, -3.9082e-01,\n",
       "                        2.3002e-01,  1.5786e-02, -1.2519e-01,  5.5482e-01, -2.7075e-01,\n",
       "                        1.3094e-02, -7.2170e-02,  1.8341e-03, -8.5223e-02,  7.7159e-03,\n",
       "                       -3.0173e-02,  5.6188e-02,  2.7987e-01,  2.2084e-01, -4.7378e-01,\n",
       "                       -3.0353e-02, -1.4822e-01, -3.5095e-01,  3.5921e-01, -3.3229e-02,\n",
       "                       -2.2243e-01,  2.0088e-01, -2.0622e-01,  1.0033e-02, -3.3748e-01,\n",
       "                        6.4750e-02, -6.3620e-02],\n",
       "                      [-4.1156e-01, -1.9112e-01,  2.2870e-01,  2.1598e-01, -1.0535e-02,\n",
       "                        7.6662e-02, -8.7347e-02, -1.1143e-01,  2.7922e-01, -9.3367e-02,\n",
       "                       -2.9914e-01, -3.0783e-01,  9.2305e-02, -1.9867e-01,  5.9002e-01,\n",
       "                        1.2712e-02, -2.0757e-01,  2.5057e-03, -2.4672e-01, -3.2041e-02,\n",
       "                        9.1949e-02, -1.0199e-01,  3.9901e-01,  1.0763e-01,  1.8202e-02,\n",
       "                       -1.1405e-01,  4.4327e-01,  2.9572e-02, -3.8114e-01, -2.3501e-01,\n",
       "                       -3.9205e-01, -3.7730e-01],\n",
       "                      [-3.6891e-01,  1.5913e-01, -9.2283e-02, -3.8851e-03, -1.0125e-01,\n",
       "                       -5.3490e-01,  2.4234e-01, -3.3741e-01,  5.4065e-02, -3.2773e-01,\n",
       "                        3.4789e-01,  1.2058e-01,  2.5072e-02,  3.1860e-01,  1.5936e-01,\n",
       "                        3.1721e-01, -5.2090e-02, -1.1264e-01,  4.8201e-02, -2.8575e-01,\n",
       "                        2.5540e-01,  1.0574e-02, -2.5219e-01, -2.2614e-01, -3.4891e-01,\n",
       "                        3.8936e-01,  2.7896e-01,  7.2532e-03,  2.4737e-01,  1.4497e-01,\n",
       "                        5.9661e-02, -4.1305e-01],\n",
       "                      [ 2.0571e-02, -1.9100e-01, -7.2230e-02,  2.2537e-01, -3.2966e-01,\n",
       "                       -1.4746e-01, -1.0134e-01,  7.5936e-02,  2.3046e-01,  7.3356e-02,\n",
       "                        6.1123e-03,  3.4040e-01,  6.1425e-01,  2.4324e-01, -2.7914e-01,\n",
       "                        2.5148e-01, -1.8915e-01, -3.7871e-03,  2.1524e-02,  2.0411e-01,\n",
       "                       -9.8206e-02, -1.1627e-01, -7.1841e-02, -4.3628e-02,  4.3795e-01,\n",
       "                       -3.3081e-01, -2.2450e-01, -3.1202e-01, -2.5061e-01,  7.6602e-02,\n",
       "                        2.0830e-01, -5.8258e-01],\n",
       "                      [-3.2612e-02,  2.7017e-01,  1.6972e-01, -3.4581e-02,  1.2357e-01,\n",
       "                       -3.4653e-01, -4.9093e-01, -1.4127e-02, -2.1082e-01,  9.3412e-02,\n",
       "                       -3.4421e-01, -9.3607e-02,  7.2965e-02,  1.1887e-01, -1.4554e-01,\n",
       "                        2.3706e-01,  1.5074e-01,  3.0441e-01,  5.4755e-01,  1.0431e-01,\n",
       "                       -1.1015e-01, -1.7963e-01,  9.4604e-02,  3.5877e-01, -7.2780e-02,\n",
       "                        2.5650e-01, -4.0245e-02, -3.4036e-01,  1.0604e-01,  2.3778e-01,\n",
       "                       -6.1080e-01, -6.7676e-02],\n",
       "                      [ 1.6799e-02,  5.2523e-01, -4.1892e-01, -5.2771e-02, -4.3528e-02,\n",
       "                       -1.0307e-01,  3.2874e-02, -1.5463e-01, -3.8645e-01, -2.2206e-01,\n",
       "                       -5.3243e-01, -1.3849e-01, -1.8205e-01,  4.3128e-01,  3.4016e-01,\n",
       "                       -7.0650e-02, -1.3617e-01,  9.7414e-02, -2.6537e-01, -1.7763e-01,\n",
       "                       -5.5483e-01, -6.7608e-02,  2.5266e-02, -6.5369e-02,  3.0631e-01,\n",
       "                       -1.2370e-01, -1.9174e-02,  2.1669e-04, -1.3437e-01,  1.0330e-01,\n",
       "                        2.8397e-01, -2.6885e-02],\n",
       "                      [-3.4560e-01,  2.6560e-02, -1.6175e-01, -8.8353e-02,  1.1566e-01,\n",
       "                       -1.2812e-01, -4.0081e-01,  1.0846e-01,  5.4816e-01, -1.0317e-01,\n",
       "                        1.4411e-01,  9.5444e-02, -4.0483e-01,  2.5015e-01, -1.8348e-01,\n",
       "                        2.5964e-01, -1.9624e-01,  1.8399e-01, -4.4645e-01,  2.8309e-01,\n",
       "                        7.2924e-02,  1.9609e-01,  2.0325e-01,  4.3294e-01,  6.3699e-02,\n",
       "                       -2.9116e-02,  1.8416e-01, -2.4917e-01,  1.2805e-01,  1.0263e-01,\n",
       "                        2.8359e-01,  3.7755e-01],\n",
       "                      [ 3.2088e-02, -1.2251e-01,  4.4453e-01,  2.9743e-02,  1.7537e-02,\n",
       "                        1.2132e-01,  5.3092e-01,  1.8737e-01,  7.4483e-02, -8.8563e-02,\n",
       "                        1.9277e-02, -4.5306e-01, -1.9043e-01,  4.7862e-01, -4.0429e-01,\n",
       "                        8.4665e-02, -1.6240e-01, -4.3579e-02,  1.1788e-01, -6.1571e-02,\n",
       "                       -7.1534e-02, -2.1737e-01,  2.7024e-01,  3.7017e-01,  3.5456e-01,\n",
       "                        1.8654e-01,  1.3651e-03,  4.6888e-01,  1.7160e-01,  1.6879e-01,\n",
       "                        5.2714e-02, -2.2139e-01],\n",
       "                      [-3.3176e-01, -6.6215e-02,  1.3895e-01,  8.5469e-02, -2.2199e-01,\n",
       "                        5.4277e-01,  1.6542e-01,  2.5174e-02, -1.1926e-01,  1.8397e-01,\n",
       "                       -3.6480e-01,  1.8529e-01,  3.8378e-01,  1.5864e-01, -2.2765e-02,\n",
       "                        7.1941e-02,  2.1980e-01, -2.8177e-01,  6.8229e-02, -4.4710e-02,\n",
       "                       -2.7068e-01,  4.9051e-01, -2.2540e-01,  3.0151e-01, -1.9722e-01,\n",
       "                        6.2168e-02,  5.2798e-01, -1.3716e-01,  1.6401e-01,  3.0909e-01,\n",
       "                        1.2935e-01,  1.5487e-01],\n",
       "                      [-1.0008e-01, -1.3340e-01,  1.7132e-01,  1.7500e-02, -3.2413e-01,\n",
       "                        1.3333e-01, -3.9212e-01,  1.3218e-01, -2.8467e-01, -5.0200e-01,\n",
       "                        2.3386e-01,  4.8619e-02,  6.8238e-02,  5.1684e-01,  2.7227e-01,\n",
       "                       -2.9752e-01, -2.7249e-01,  8.9261e-02,  1.6182e-01,  3.9764e-01,\n",
       "                        2.4940e-02,  2.6525e-01, -3.3338e-01,  1.7955e-01, -1.3938e-01,\n",
       "                       -6.1873e-02, -2.2041e-01,  3.6102e-01, -1.9394e-02, -3.1389e-01,\n",
       "                       -1.9317e-01,  6.8371e-02],\n",
       "                      [ 1.2645e-01,  2.4378e-02, -2.2781e-01, -4.9132e-01, -2.6346e-02,\n",
       "                       -2.8866e-01,  2.4808e-01, -4.6752e-01,  4.5194e-02,  1.6585e-01,\n",
       "                       -2.0504e-01,  4.9931e-01,  9.7349e-02,  9.9132e-02, -1.3775e-01,\n",
       "                       -4.6768e-01, -2.8464e-01, -1.8994e-01,  2.1707e-01,  1.4220e-01,\n",
       "                        2.4917e-01,  2.4619e-01,  4.8330e-01,  2.8794e-01, -2.3489e-03,\n",
       "                       -1.1363e-01,  1.3105e-01,  2.4256e-01, -1.0167e-01,  1.4712e-03,\n",
       "                       -1.5868e-01,  3.7644e-03],\n",
       "                      [ 3.5874e-01, -4.5782e-02, -3.1940e-01,  2.3702e-01, -6.5051e-02,\n",
       "                       -1.5597e-01, -9.5117e-02, -1.8051e-01,  4.4031e-01,  2.9589e-02,\n",
       "                       -1.7037e-01, -3.5579e-01, -2.1074e-01, -2.3445e-01, -1.9725e-01,\n",
       "                       -1.0646e-01,  9.6421e-02,  4.1068e-03,  4.9895e-02,  4.7207e-01,\n",
       "                       -4.1591e-01,  5.0237e-01, -2.6876e-01, -6.0585e-02, -1.0551e-01,\n",
       "                        3.3990e-01,  1.3580e-01,  2.9199e-01, -1.5700e-02, -1.3792e-02,\n",
       "                       -2.4712e-02, -4.1654e-01]], device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_linear_layer_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_layer_norm_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "                     device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_layer_norm_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_linear_layer_output.weight',\n",
       "              tensor([[ 0.0184, -0.0454, -0.0361,  0.0252,  0.0644,  0.0251,  0.0382,  0.0006,\n",
       "                        0.0033,  0.0088,  0.0326, -0.0458,  0.0097, -0.0562, -0.0317,  0.0438],\n",
       "                      [-0.0247,  0.0385, -0.0544,  0.0617, -0.0151, -0.0207,  0.0282, -0.0078,\n",
       "                        0.0173, -0.0472, -0.0198,  0.0494,  0.0123, -0.0238, -0.0562, -0.0199],\n",
       "                      [-0.0498,  0.0315,  0.0396, -0.0606,  0.0587, -0.0039,  0.0120,  0.0128,\n",
       "                       -0.0122, -0.0247, -0.0627, -0.0113, -0.0044, -0.0469, -0.0209, -0.0073],\n",
       "                      [-0.0083, -0.0331, -0.0049,  0.0142,  0.0253,  0.0091, -0.0356,  0.0341,\n",
       "                        0.0961, -0.0304, -0.0008, -0.0007, -0.0253, -0.0222,  0.0439, -0.0473]],\n",
       "                     device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_linear_layer_output.bias',\n",
       "              tensor([0., 0., 0., 0.], device='cuda:0'))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_dict = agent_pop[0].actor.state_dict()\n",
    "before_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.model.encoder_conv_layer_1.weight',\n",
       "              tensor([[[[ 1.2011e-01, -2.3543e-01, -8.6037e-03,  3.5908e-02],\n",
       "                        [-1.8133e-02, -9.5431e-02,  1.5800e-01,  1.9248e-01],\n",
       "                        [ 1.8469e-01,  2.1109e-01, -2.7355e-03, -8.7567e-02],\n",
       "                        [-3.3392e-01, -2.5585e-01,  1.8480e-01, -1.3048e-01]],\n",
       "              \n",
       "                       [[-1.0107e-01, -7.1645e-02,  1.5379e-02, -1.8511e-03],\n",
       "                        [-1.1292e-01,  5.4702e-02, -2.5859e-01, -5.7284e-02],\n",
       "                        [-2.7937e-01,  5.6650e-02, -1.2157e-01, -3.0416e-01],\n",
       "                        [ 1.5155e-01, -2.0830e-01,  3.1269e-01, -2.5233e-02]],\n",
       "              \n",
       "                       [[-4.8939e-02,  8.9096e-02,  5.2152e-02,  1.3511e-01],\n",
       "                        [-1.1432e-05, -1.7608e-01, -2.5988e-01, -5.6473e-02],\n",
       "                        [-3.7252e-01,  1.8827e-01,  1.0439e-01,  3.1282e-01],\n",
       "                        [-1.8274e-01,  2.8279e-01,  3.5791e-01,  2.8038e-01]],\n",
       "              \n",
       "                       [[ 8.9804e-02, -5.8677e-02,  2.7004e-02, -7.0852e-02],\n",
       "                        [-2.8230e-02,  4.3004e-02, -2.4714e-02, -1.7923e-01],\n",
       "                        [ 1.6370e-01,  3.4152e-02,  7.9987e-02, -4.1203e-01],\n",
       "                        [-2.3748e-01,  1.2957e-01,  5.5091e-02,  1.3773e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1107e-01,  7.2074e-02, -3.1453e-01,  6.2464e-03],\n",
       "                        [ 5.4855e-02, -3.7036e-01, -3.3700e-01,  3.1816e-02],\n",
       "                        [ 6.3167e-01,  3.0748e-02,  2.8541e-01, -1.9061e-01],\n",
       "                        [ 9.9318e-02, -9.2246e-02,  1.2380e-01, -1.1617e-01]],\n",
       "              \n",
       "                       [[-9.1376e-02,  5.3209e-02, -3.1894e-02, -2.4113e-01],\n",
       "                        [-1.5884e-01, -7.3952e-02,  1.8294e-01, -4.6125e-03],\n",
       "                        [-1.2181e-01, -7.0672e-02, -1.2083e-01,  1.9893e-01],\n",
       "                        [ 2.9923e-01,  3.0868e-01, -1.4328e-01,  1.5708e-03]],\n",
       "              \n",
       "                       [[-8.3678e-02, -3.0063e-02,  3.7526e-02, -9.8041e-02],\n",
       "                        [-1.3222e-01, -2.1051e-03, -5.0228e-02,  2.1036e-01],\n",
       "                        [ 2.1694e-01, -1.9992e-01, -3.2453e-02,  2.0238e-01],\n",
       "                        [ 1.4789e-01, -1.1277e-01,  1.7869e-01,  7.1963e-02]],\n",
       "              \n",
       "                       [[ 9.9843e-02, -1.9505e-01,  7.7674e-02, -2.7082e-03],\n",
       "                        [ 3.9029e-02, -1.5873e-01, -2.3818e-01,  1.2174e-01],\n",
       "                        [ 1.9924e-01, -2.4638e-02, -2.7554e-03,  4.0205e-02],\n",
       "                        [ 1.0913e-01, -1.5696e-01,  3.6385e-01,  5.3011e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.9041e-02, -1.6014e-01,  9.2199e-02, -6.8461e-02],\n",
       "                        [ 1.8724e-01,  8.0130e-02,  1.5080e-01, -4.2931e-02],\n",
       "                        [ 1.6475e-01,  1.3786e-01, -1.0139e-01, -1.7455e-01],\n",
       "                        [-2.2143e-01,  5.8592e-02, -8.4089e-02,  2.0014e-01]],\n",
       "              \n",
       "                       [[ 2.4926e-01, -1.7050e-01, -3.1458e-01, -1.2425e-01],\n",
       "                        [-1.6455e-01, -4.1371e-01, -1.7275e-01,  1.1836e-01],\n",
       "                        [ 2.8510e-01, -1.0710e-01,  2.6969e-02, -2.2883e-01],\n",
       "                        [ 1.0387e-01,  7.2202e-02, -1.7806e-01,  7.9270e-02]],\n",
       "              \n",
       "                       [[ 2.7374e-01, -1.8773e-01,  2.9003e-01, -4.6847e-02],\n",
       "                        [-4.4702e-02, -1.2204e-01, -3.0922e-01,  4.8348e-02],\n",
       "                        [ 2.5525e-01, -4.3556e-02,  1.9179e-01, -5.6015e-02],\n",
       "                        [ 6.5454e-03,  4.8759e-02, -6.5314e-02, -4.1482e-01]],\n",
       "              \n",
       "                       [[-1.3044e-01, -1.2428e-01,  2.1591e-02, -1.6670e-01],\n",
       "                        [-2.0495e-01, -1.0260e-01, -1.6921e-01, -2.9909e-01],\n",
       "                        [ 2.8477e-01, -6.2529e-02, -9.2274e-03,  1.4084e-01],\n",
       "                        [ 6.0233e-02,  2.5500e-01, -1.7998e-01,  1.5873e-01]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 9.4267e-02, -1.2810e-01,  6.6303e-02,  6.0780e-02],\n",
       "                        [ 5.0643e-02, -2.0143e-01, -3.4427e-02,  1.1047e-01],\n",
       "                        [-4.0412e-02, -1.0708e-01, -2.1723e-01, -1.2981e-01],\n",
       "                        [-3.8459e-02,  8.9650e-02,  1.5192e-01, -5.7997e-02]],\n",
       "              \n",
       "                       [[-3.3811e-01, -4.0965e-03, -2.8274e-01,  9.5860e-02],\n",
       "                        [-2.5655e-01, -3.1848e-02,  1.9202e-01, -4.4868e-01],\n",
       "                        [ 2.9863e-01,  2.0459e-01,  1.7338e-02,  1.9129e-01],\n",
       "                        [-2.4265e-01, -8.4460e-02,  2.4479e-01, -9.9731e-02]],\n",
       "              \n",
       "                       [[ 1.6892e-01, -1.4194e-02, -5.9123e-03,  1.3460e-01],\n",
       "                        [-1.7432e-01, -2.9581e-01,  1.5870e-01, -1.2473e-01],\n",
       "                        [ 1.1753e-01,  6.1209e-02,  1.3639e-01, -8.4989e-02],\n",
       "                        [-9.9626e-02, -9.3963e-02, -4.9895e-02,  3.3045e-01]],\n",
       "              \n",
       "                       [[-2.3058e-01, -3.5004e-01, -9.5699e-02, -3.8078e-01],\n",
       "                        [-1.2707e-01,  2.0823e-01, -1.3419e-01,  1.4048e-01],\n",
       "                        [ 1.2921e-01, -1.1034e-01,  1.1983e-01,  2.0239e-01],\n",
       "                        [-9.9487e-03, -2.5035e-01, -1.4390e-01, -1.0286e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.7585e-01, -2.6824e-01,  1.7037e-01, -1.9229e-01],\n",
       "                        [ 1.1353e-01,  2.3247e-01, -2.2408e-01,  1.7776e-01],\n",
       "                        [ 4.1535e-01,  1.5175e-01, -9.0450e-02, -3.5494e-01],\n",
       "                        [-3.0910e-02,  3.3555e-01, -1.0437e-01, -9.9014e-03]],\n",
       "              \n",
       "                       [[ 1.1647e-01, -3.5236e-02, -2.1880e-02,  2.3412e-01],\n",
       "                        [-2.0037e-01,  1.4811e-01, -1.6062e-01,  1.0957e-01],\n",
       "                        [-1.4924e-01,  1.3139e-02,  6.6558e-02, -2.4374e-01],\n",
       "                        [ 2.1279e-01,  2.1419e-02, -1.1269e-02,  5.7992e-02]],\n",
       "              \n",
       "                       [[-3.0696e-01,  7.9110e-03, -1.8179e-01,  2.5534e-01],\n",
       "                        [-9.8161e-02, -2.1899e-01,  2.2034e-01, -2.7321e-01],\n",
       "                        [ 1.8026e-01,  5.3833e-02, -5.6990e-02, -1.7128e-01],\n",
       "                        [ 4.6562e-03, -9.3539e-03, -3.9721e-01, -2.3138e-02]],\n",
       "              \n",
       "                       [[ 2.7893e-01, -1.1527e-01, -2.8283e-01,  7.3017e-02],\n",
       "                        [-1.2532e-02, -5.8219e-02,  2.2593e-01,  1.0784e-01],\n",
       "                        [-1.0642e-01,  6.9503e-04,  9.0144e-02, -1.2831e-01],\n",
       "                        [ 4.7002e-02, -1.4687e-01,  5.4863e-02, -1.3256e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.3627e-02,  2.9010e-01,  1.4430e-01, -6.7833e-02],\n",
       "                        [ 1.0944e-01,  2.6750e-01,  1.0960e-02,  2.8953e-01],\n",
       "                        [ 1.0643e-02, -1.7182e-01,  1.2802e-01, -6.9147e-02],\n",
       "                        [ 9.4883e-05, -1.0955e-01, -1.4471e-01, -1.8639e-01]],\n",
       "              \n",
       "                       [[-2.1994e-01, -2.3579e-01,  5.5414e-02, -2.7230e-02],\n",
       "                        [ 1.8455e-02, -5.2442e-02,  1.1061e-01,  2.6582e-02],\n",
       "                        [ 2.9508e-02,  8.6890e-02, -2.8366e-03,  2.5296e-01],\n",
       "                        [-1.1167e-01,  5.3331e-02,  9.8459e-03,  7.4608e-02]],\n",
       "              \n",
       "                       [[ 5.3238e-03, -8.3735e-02, -6.3682e-02,  2.2960e-01],\n",
       "                        [ 1.5238e-01,  7.7715e-02, -3.1816e-01, -5.3728e-02],\n",
       "                        [ 1.8861e-01,  7.3357e-02,  1.4209e-01,  3.0953e-01],\n",
       "                        [-5.0695e-01, -1.8679e-01,  1.1660e-01, -1.8949e-02]],\n",
       "              \n",
       "                       [[ 3.7341e-01,  2.7684e-02, -4.2918e-01, -1.2789e-01],\n",
       "                        [-5.9681e-02,  3.9809e-02,  1.2472e-01,  3.6296e-02],\n",
       "                        [-4.1718e-03,  3.6855e-01, -1.7901e-01,  1.7528e-01],\n",
       "                        [ 3.1612e-01,  2.1096e-01,  1.8753e-02,  1.4458e-01]]]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_conv_layer_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_layer_norm_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_layer_norm_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_layer_norm_1.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_layer_norm_1.running_var',\n",
       "              tensor([0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.model.encoder_layer_norm_1.num_batches_tracked',\n",
       "              tensor(2, device='cuda:0')),\n",
       "             ('encoder.model.encoder_linear_output.weight',\n",
       "              tensor([[-2.6982e-03,  8.8416e-04, -1.6105e-03,  ..., -4.2469e-04,\n",
       "                        3.0841e-03,  8.7087e-04],\n",
       "                      [-1.0762e-03,  2.9336e-03,  1.1011e-03,  ...,  1.1705e-03,\n",
       "                        1.9205e-03,  1.7049e-03],\n",
       "                      [ 2.1716e-03, -3.7796e-04, -9.7582e-04,  ..., -1.4043e-03,\n",
       "                       -8.7901e-04,  2.1939e-04],\n",
       "                      ...,\n",
       "                      [ 2.3385e-03, -6.7821e-04, -6.1678e-04,  ...,  2.5169e-03,\n",
       "                        6.2754e-04, -2.3831e-03],\n",
       "                      [ 2.5204e-03, -2.0914e-04,  5.1959e-04,  ..., -2.9482e-03,\n",
       "                        2.4382e-03,  2.7641e-04],\n",
       "                      [ 3.9437e-04, -6.7143e-05, -1.6697e-03,  ..., -1.0852e-03,\n",
       "                       -2.4008e-04, -2.8168e-03]], device='cuda:0')),\n",
       "             ('encoder.model.encoder_linear_output.bias',\n",
       "              tensor([ 0.0022,  0.0020, -0.0021,  0.0013,  0.0014, -0.0027, -0.0001, -0.0027,\n",
       "                       0.0010,  0.0007, -0.0026,  0.0004,  0.0025, -0.0002,  0.0019,  0.0008,\n",
       "                      -0.0021,  0.0005, -0.0020,  0.0019, -0.0011, -0.0021, -0.0022, -0.0023,\n",
       "                       0.0015, -0.0022, -0.0029, -0.0010, -0.0029, -0.0025,  0.0016, -0.0015],\n",
       "                     device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_linear_layer_1.weight',\n",
       "              tensor([[ 1.9515e-01, -1.6597e-01, -4.0421e-02,  1.7122e-01, -9.1142e-03,\n",
       "                        9.4596e-02, -5.9549e-01, -1.1075e-01,  1.4865e-01, -2.4306e-01,\n",
       "                       -6.8971e-02,  1.3106e-01, -4.1923e-01,  1.8785e-01,  1.1577e-02,\n",
       "                       -3.8778e-02,  4.0489e-01, -7.6712e-01,  1.9766e-01, -3.0815e-01,\n",
       "                        1.7862e-01, -1.4948e-01,  2.6292e-02, -1.9560e-02,  1.4881e-01,\n",
       "                       -2.4611e-01,  5.8229e-02,  1.7403e-01, -1.1102e-01,  3.6546e-01,\n",
       "                       -2.3131e-02, -8.4586e-02],\n",
       "                      [ 3.2324e-01, -2.9664e-01,  2.7525e-01,  1.4115e-01, -4.2086e-01,\n",
       "                       -1.6181e-01,  2.6927e-01, -1.2423e-01, -6.1818e-02,  1.2901e-01,\n",
       "                       -2.3235e-01,  1.4302e-01, -4.1033e-01,  4.9666e-02,  3.9880e-01,\n",
       "                       -1.6450e-01,  1.0063e-01,  3.0905e-01, -2.0553e-01,  2.8991e-01,\n",
       "                        3.7790e-01, -7.9027e-03, -2.4783e-01, -7.5432e-02,  3.5815e-01,\n",
       "                       -5.6521e-02,  1.0957e-01, -3.0360e-01,  3.7955e-01,  3.5437e-01,\n",
       "                       -1.4347e-01,  3.1266e-02],\n",
       "                      [ 1.4282e-02,  5.1707e-01,  1.2399e-01,  1.7680e-01, -1.0161e-01,\n",
       "                       -3.5483e-01, -4.9666e-02,  1.5923e-01,  1.9858e-01,  4.3562e-01,\n",
       "                        1.2665e-01, -4.2984e-02,  2.7459e-01, -1.0000e-01,  3.9929e-01,\n",
       "                       -1.1605e-01,  1.7680e-02, -5.9203e-01, -1.6694e-01,  3.6475e-02,\n",
       "                        3.3514e-02, -8.3827e-02, -2.2107e-01,  4.2061e-01,  3.7079e-01,\n",
       "                        2.8971e-01, -1.7712e-01,  1.3626e-01,  2.0414e-01, -2.3689e-01,\n",
       "                        9.3902e-03,  1.8214e-01],\n",
       "                      [ 2.8718e-01, -1.9452e-01, -2.7487e-01,  7.0442e-02, -3.4681e-01,\n",
       "                        2.0146e-01, -1.9128e-01,  3.4341e-01, -2.4071e-01, -6.5058e-02,\n",
       "                       -2.4306e-01,  8.9122e-02, -1.4885e-01, -1.0703e-01,  2.8016e-02,\n",
       "                        9.9423e-02, -3.6061e-01, -1.9426e-01,  1.0413e-02, -2.6461e-01,\n",
       "                        1.7129e-01,  1.1770e-01,  4.5449e-01,  9.3120e-02, -1.2538e-01,\n",
       "                        5.1206e-01, -1.1480e-01, -3.1633e-01,  4.1293e-01, -3.6830e-01,\n",
       "                        1.8147e-01, -2.8938e-01],\n",
       "                      [ 4.1197e-01,  3.0449e-01,  1.3845e-01, -4.6537e-01, -3.9082e-01,\n",
       "                        2.3002e-01,  1.5786e-02, -1.2519e-01,  5.5482e-01, -2.7075e-01,\n",
       "                        1.3094e-02, -7.2170e-02,  1.8341e-03, -8.5223e-02,  7.7159e-03,\n",
       "                       -3.0173e-02,  5.6188e-02,  2.7987e-01,  2.2084e-01, -4.7378e-01,\n",
       "                       -3.0353e-02, -1.4822e-01, -3.5095e-01,  3.5921e-01, -3.3229e-02,\n",
       "                       -2.2243e-01,  2.0088e-01, -2.0622e-01,  1.0033e-02, -3.3748e-01,\n",
       "                        6.4750e-02, -6.3620e-02],\n",
       "                      [-4.1156e-01, -1.9112e-01,  2.2870e-01,  2.1598e-01, -1.0535e-02,\n",
       "                        7.6662e-02, -8.7347e-02, -1.1143e-01,  2.7922e-01, -9.3367e-02,\n",
       "                       -2.9914e-01, -3.0783e-01,  9.2305e-02, -1.9867e-01,  5.9002e-01,\n",
       "                        1.2712e-02, -2.0757e-01,  2.5057e-03, -2.4672e-01, -3.2041e-02,\n",
       "                        9.1949e-02, -1.0199e-01,  3.9901e-01,  1.0763e-01,  1.8202e-02,\n",
       "                       -1.1405e-01,  4.4327e-01,  2.9572e-02, -3.8114e-01, -2.3501e-01,\n",
       "                       -3.9205e-01, -3.7730e-01],\n",
       "                      [-3.6891e-01,  1.5913e-01, -9.2283e-02, -3.8851e-03, -1.0125e-01,\n",
       "                       -5.3490e-01,  2.4234e-01, -3.3741e-01,  5.4065e-02, -3.2773e-01,\n",
       "                        3.4789e-01,  1.2058e-01,  2.5072e-02,  3.1860e-01,  1.5936e-01,\n",
       "                        3.1721e-01, -5.2090e-02, -1.1264e-01,  4.8201e-02, -2.8575e-01,\n",
       "                        2.5540e-01,  1.0574e-02, -2.5219e-01, -2.2614e-01, -3.4891e-01,\n",
       "                        3.8936e-01,  2.7896e-01,  7.2532e-03,  2.4737e-01,  1.4497e-01,\n",
       "                        5.9661e-02, -4.1305e-01],\n",
       "                      [ 2.0571e-02, -1.9100e-01, -7.2230e-02,  2.2537e-01, -3.2966e-01,\n",
       "                       -1.4746e-01, -1.0134e-01,  7.5936e-02,  2.3046e-01,  7.3356e-02,\n",
       "                        6.1123e-03,  3.4040e-01,  6.1425e-01,  2.4324e-01, -2.7914e-01,\n",
       "                        2.5148e-01, -1.8915e-01, -3.7871e-03,  2.1524e-02,  2.0411e-01,\n",
       "                       -9.8206e-02, -1.1627e-01, -7.1841e-02, -4.3628e-02,  4.3795e-01,\n",
       "                       -3.3081e-01, -2.2450e-01, -3.1202e-01, -2.5061e-01,  7.6602e-02,\n",
       "                        2.0830e-01, -5.8258e-01],\n",
       "                      [-3.2612e-02,  2.7017e-01,  1.6972e-01, -3.4581e-02,  1.2357e-01,\n",
       "                       -3.4653e-01, -4.9093e-01, -1.4127e-02, -2.1082e-01,  9.3412e-02,\n",
       "                       -3.4421e-01, -9.3607e-02,  7.2965e-02,  1.1887e-01, -1.4554e-01,\n",
       "                        2.3706e-01,  1.5074e-01,  3.0441e-01,  5.4755e-01,  1.0431e-01,\n",
       "                       -1.1015e-01, -1.7963e-01,  9.4604e-02,  3.5877e-01, -7.2780e-02,\n",
       "                        2.5650e-01, -4.0245e-02, -3.4036e-01,  1.0604e-01,  2.3778e-01,\n",
       "                       -6.1080e-01, -6.7676e-02],\n",
       "                      [ 1.6799e-02,  5.2523e-01, -4.1892e-01, -5.2771e-02, -4.3528e-02,\n",
       "                       -1.0307e-01,  3.2874e-02, -1.5463e-01, -3.8645e-01, -2.2206e-01,\n",
       "                       -5.3243e-01, -1.3849e-01, -1.8205e-01,  4.3128e-01,  3.4016e-01,\n",
       "                       -7.0650e-02, -1.3617e-01,  9.7414e-02, -2.6537e-01, -1.7763e-01,\n",
       "                       -5.5483e-01, -6.7608e-02,  2.5266e-02, -6.5369e-02,  3.0631e-01,\n",
       "                       -1.2370e-01, -1.9174e-02,  2.1669e-04, -1.3437e-01,  1.0330e-01,\n",
       "                        2.8397e-01, -2.6885e-02],\n",
       "                      [-3.4560e-01,  2.6560e-02, -1.6175e-01, -8.8353e-02,  1.1566e-01,\n",
       "                       -1.2812e-01, -4.0081e-01,  1.0846e-01,  5.4816e-01, -1.0317e-01,\n",
       "                        1.4411e-01,  9.5444e-02, -4.0483e-01,  2.5015e-01, -1.8348e-01,\n",
       "                        2.5964e-01, -1.9624e-01,  1.8399e-01, -4.4645e-01,  2.8309e-01,\n",
       "                        7.2924e-02,  1.9609e-01,  2.0325e-01,  4.3294e-01,  6.3699e-02,\n",
       "                       -2.9116e-02,  1.8416e-01, -2.4917e-01,  1.2805e-01,  1.0263e-01,\n",
       "                        2.8359e-01,  3.7755e-01],\n",
       "                      [ 3.2088e-02, -1.2251e-01,  4.4453e-01,  2.9743e-02,  1.7537e-02,\n",
       "                        1.2132e-01,  5.3092e-01,  1.8737e-01,  7.4483e-02, -8.8563e-02,\n",
       "                        1.9277e-02, -4.5306e-01, -1.9043e-01,  4.7862e-01, -4.0429e-01,\n",
       "                        8.4665e-02, -1.6240e-01, -4.3579e-02,  1.1788e-01, -6.1571e-02,\n",
       "                       -7.1534e-02, -2.1737e-01,  2.7024e-01,  3.7017e-01,  3.5456e-01,\n",
       "                        1.8654e-01,  1.3651e-03,  4.6888e-01,  1.7160e-01,  1.6879e-01,\n",
       "                        5.2714e-02, -2.2139e-01],\n",
       "                      [-3.3176e-01, -6.6215e-02,  1.3895e-01,  8.5469e-02, -2.2199e-01,\n",
       "                        5.4277e-01,  1.6542e-01,  2.5174e-02, -1.1926e-01,  1.8397e-01,\n",
       "                       -3.6480e-01,  1.8529e-01,  3.8378e-01,  1.5864e-01, -2.2765e-02,\n",
       "                        7.1941e-02,  2.1980e-01, -2.8177e-01,  6.8229e-02, -4.4710e-02,\n",
       "                       -2.7068e-01,  4.9051e-01, -2.2540e-01,  3.0151e-01, -1.9722e-01,\n",
       "                        6.2168e-02,  5.2798e-01, -1.3716e-01,  1.6401e-01,  3.0909e-01,\n",
       "                        1.2935e-01,  1.5487e-01],\n",
       "                      [-1.0008e-01, -1.3340e-01,  1.7132e-01,  1.7500e-02, -3.2413e-01,\n",
       "                        1.3333e-01, -3.9212e-01,  1.3218e-01, -2.8467e-01, -5.0200e-01,\n",
       "                        2.3386e-01,  4.8619e-02,  6.8238e-02,  5.1684e-01,  2.7227e-01,\n",
       "                       -2.9752e-01, -2.7249e-01,  8.9261e-02,  1.6182e-01,  3.9764e-01,\n",
       "                        2.4940e-02,  2.6525e-01, -3.3338e-01,  1.7955e-01, -1.3938e-01,\n",
       "                       -6.1873e-02, -2.2041e-01,  3.6102e-01, -1.9394e-02, -3.1389e-01,\n",
       "                       -1.9317e-01,  6.8371e-02],\n",
       "                      [ 1.2645e-01,  2.4378e-02, -2.2781e-01, -4.9132e-01, -2.6346e-02,\n",
       "                       -2.8866e-01,  2.4808e-01, -4.6752e-01,  4.5194e-02,  1.6585e-01,\n",
       "                       -2.0504e-01,  4.9931e-01,  9.7349e-02,  9.9132e-02, -1.3775e-01,\n",
       "                       -4.6768e-01, -2.8464e-01, -1.8994e-01,  2.1707e-01,  1.4220e-01,\n",
       "                        2.4917e-01,  2.4619e-01,  4.8330e-01,  2.8794e-01, -2.3489e-03,\n",
       "                       -1.1363e-01,  1.3105e-01,  2.4256e-01, -1.0167e-01,  1.4712e-03,\n",
       "                       -1.5868e-01,  3.7644e-03],\n",
       "                      [ 3.5874e-01, -4.5782e-02, -3.1940e-01,  2.3702e-01, -6.5051e-02,\n",
       "                       -1.5597e-01, -9.5117e-02, -1.8051e-01,  4.4031e-01,  2.9589e-02,\n",
       "                       -1.7037e-01, -3.5579e-01, -2.1074e-01, -2.3445e-01, -1.9725e-01,\n",
       "                       -1.0646e-01,  9.6421e-02,  4.1068e-03,  4.9895e-02,  4.7207e-01,\n",
       "                       -4.1591e-01,  5.0237e-01, -2.6876e-01, -6.0585e-02, -1.0551e-01,\n",
       "                        3.3990e-01,  1.3580e-01,  2.9199e-01, -1.5700e-02, -1.3792e-02,\n",
       "                       -2.4712e-02, -4.1654e-01]], device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_linear_layer_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_layer_norm_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "                     device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_layer_norm_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_linear_layer_output.weight',\n",
       "              tensor([[ 0.0184, -0.0454, -0.0361,  0.0252,  0.0644,  0.0251,  0.0382,  0.0006,\n",
       "                        0.0033,  0.0088,  0.0326, -0.0458,  0.0097, -0.0562, -0.0317,  0.0438],\n",
       "                      [-0.0247,  0.0385, -0.0544,  0.0617, -0.0151, -0.0207,  0.0282, -0.0078,\n",
       "                        0.0173, -0.0472, -0.0198,  0.0494,  0.0123, -0.0238, -0.0562, -0.0199],\n",
       "                      [-0.0498,  0.0315,  0.0396, -0.0606,  0.0587, -0.0039,  0.0120,  0.0128,\n",
       "                       -0.0122, -0.0247, -0.0627, -0.0113, -0.0044, -0.0469, -0.0209, -0.0073],\n",
       "                      [-0.0083, -0.0331, -0.0049,  0.0142,  0.0253,  0.0091, -0.0356,  0.0341,\n",
       "                        0.0961, -0.0304, -0.0008, -0.0007, -0.0253, -0.0222,  0.0439, -0.0473]],\n",
       "                     device='cuda:0')),\n",
       "             ('head_net._wrapped.model.actor_linear_layer_output.bias',\n",
       "              tensor([0., 0., 0., 0.], device='cuda:0'))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = mutated_population[0]\n",
    "after_dict = ind.actor.state_dict()\n",
    "after_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_equal_state_dict(before_pop, mutated_pop):\n",
    "    for before_ind, mutated in zip(before_pop, mutated_pop):\n",
    "        before_modules = before_ind.evolvable_attributes(networks_only=True).values()\n",
    "        mutated_modules = mutated.evolvable_attributes(networks_only=True).values()\n",
    "        for before_mod, mutated_mod in zip(before_modules, mutated_modules):\n",
    "            before_dict = before_mod.state_dict()\n",
    "            after_dict = mutated_mod.state_dict()\n",
    "            for key, param in after_dict.items():\n",
    "                if key in before_dict:\n",
    "                    old_param = before_dict[key]\n",
    "                    old_size = old_param.data.size()\n",
    "                    new_size = param.data.size()\n",
    "                    if old_size == new_size:\n",
    "                        # If the sizes are the same, just copy the parameter\n",
    "                        param.data = old_param.data\n",
    "                    elif \"norm\" not in key:\n",
    "                        # Create a slicing index to handle tensors with varying sizes\n",
    "                        slice_index = tuple(slice(0, min(o, n)) for o, n in zip(old_size[:2], new_size[:2]))\n",
    "                        assert torch.all(torch.eq(param.data[slice_index], old_param.data[slice_index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.add_channel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ValueFunction(\n",
       "  (encoder): EvolvableCNN(\n",
       "    (model): Sequential(\n",
       "      (encoder_conv_layer_1): Conv2d(4, 32, kernel_size=(4, 4), stride=(1, 1))\n",
       "      (encoder_layer_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (encoder_activation_1): ReLU()\n",
       "      (encoder_flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (encoder_linear_output): Linear(in_features=209952, out_features=32, bias=True)\n",
       "      (encoder_output_activation): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (head_net): EvolvableMLP(\n",
       "    (model): Sequential(\n",
       "      (value_linear_layer_1): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (value_layer_norm_1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (value_activation_1): ReLU()\n",
       "      (value_linear_layer_output): Linear(in_features=16, out_features=1, bias=True)\n",
       "      (value_activation_output): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ind.critic.last_mutation_attr)\n",
    "ind.critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agilerl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
