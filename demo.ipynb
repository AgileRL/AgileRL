{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaimesabal/.pyenv/versions/agilerl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from agilerl.modules.cnn import EvolvableCNN\n",
    "from agilerl.modules.multi_input import EvolvableMultiInput\n",
    "from agilerl.modules.mlp import EvolvableMLP\n",
    "from agilerl.hpo.mutation import Mutations, get_return_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining an EvolvableComposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "from tests.helper_functions import generate_dict_or_tuple_space\n",
    "\n",
    "# Create a sample Dict observation space with 3 image spaces and 2 vector spaces\n",
    "observation_space = generate_dict_or_tuple_space(3, 4)\n",
    "print(type(observation_space))\n",
    "latent_dim = 10\n",
    "num_outputs = 10\n",
    "num_atoms = 51\n",
    "channel_size = (8, 16, 32)\n",
    "kernel_size = (3, 3, 3)\n",
    "stride_size = (1, 1, 1)\n",
    "hidden_size = [64, 64]\n",
    "v_min = -10\n",
    "v_max = 10\n",
    "support = torch.linspace(v_min, v_max, num_atoms)\n",
    "net = EvolvableMultiInput(\n",
    "    observation_space,\n",
    "    channel_size,\n",
    "    kernel_size,\n",
    "    stride_size,\n",
    "    hidden_size,\n",
    "    latent_dim,\n",
    "    num_outputs,\n",
    "    rainbow=True,\n",
    "    num_atoms=num_atoms,\n",
    "    support=support,\n",
    "    n_agents=2\n",
    "    )\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('image_0': Box(2.0, 7.0, (3, 128, 128), float32), 'image_1': Box(2.0, 8.0, (3, 128, 128), float32), 'image_2': Box(2.0, 8.0, (3, 128, 128), float32), 'vector_0': Box(3.0, 8.0, (5,), float32), 'vector_1': Box(5.0, 10.0, (5,), float32), 'vector_2': Box(5.0, 8.0, (5,), float32), 'vector_3': Box(4.0, 7.0, (5,), float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = observation_space.sample()\n",
    "\n",
    "if isinstance(sample, dict):\n",
    "    sample = {k: torch.tensor(sample[k]).unsqueeze(0) for k in sample}\n",
    "else:\n",
    "    sample = tuple([torch.tensor(comp).unsqueeze(0) for comp in sample])\n",
    "\n",
    "# Test forward pass\n",
    "out = net(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2286, -0.9093,  2.2788, -0.0070,  0.0133,  0.6296, -0.7523,  3.4447,\n",
       "         -2.1520, -1.9143]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probe Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "class DictObsProbeEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Base class for GridWorld-based MultiObs Environments 4x4  grid world.\n",
    "\n",
    "    .. code-block:: text\n",
    "\n",
    "        ____________\n",
    "       | 0  1  2   3|\n",
    "       | 4|¯5¯¯6¯| 7|\n",
    "       | 8|_9_10_|11|\n",
    "       |12 13  14 15|\n",
    "       ¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
    "\n",
    "    start is 0\n",
    "    states 5, 6, 9, and 10 are blocked\n",
    "    goal is 15\n",
    "    actions are = [left, down, right, up]\n",
    "\n",
    "    simple linear state env of 15 states but encoded with a vector and an image observation:\n",
    "    each column is represented by a random vector and each row is\n",
    "    represented by a random image, both sampled once at creation time.\n",
    "\n",
    "    :param num_col: Number of columns in the grid\n",
    "    :param num_row: Number of rows in the grid\n",
    "    :param random_start: If true, agent starts in random position\n",
    "    :param channel_last: If true, the image will be channel last, else it will be channel first\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_col: int = 4,\n",
    "        num_row: int = 4,\n",
    "        random_start: bool = True,\n",
    "        discrete_actions: bool = True,\n",
    "        channel_last: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vector_size = 5\n",
    "        if channel_last:\n",
    "            self.img_size = [64, 64, 1]\n",
    "        else:\n",
    "            self.img_size = [1, 64, 64]\n",
    "\n",
    "        self.random_start = random_start\n",
    "        self.discrete_actions = discrete_actions\n",
    "        if discrete_actions:\n",
    "            self.action_space = spaces.Discrete(4)\n",
    "        else:\n",
    "            self.action_space = spaces.Box(0, 1, (4,))\n",
    "\n",
    "        self.observation_space = spaces.Dict(\n",
    "            spaces={\n",
    "                \"vec\": spaces.Box(0, 1, (self.vector_size,), dtype=np.float64),\n",
    "                \"img\": spaces.Box(0, 255, self.img_size, dtype=np.uint8),\n",
    "            }\n",
    "        )\n",
    "        self.count = 0\n",
    "        # Timeout\n",
    "        self.max_count = 100\n",
    "        self.log = \"\"\n",
    "        self.state = 0\n",
    "        self.action2str = [\"left\", \"down\", \"right\", \"up\"]\n",
    "        self.init_possible_transitions()\n",
    "\n",
    "        self.num_col = num_col\n",
    "        self.state_mapping: List[Dict[str, np.ndarray]] = []\n",
    "        self.init_state_mapping(num_col, num_row)\n",
    "\n",
    "        self.max_state = len(self.state_mapping) - 1\n",
    "\n",
    "    def init_state_mapping(self, num_col: int, num_row: int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the state_mapping array which holds the observation values for each state\n",
    "\n",
    "        :param num_col: Number of columns.\n",
    "        :param num_row: Number of rows.\n",
    "        \"\"\"\n",
    "        # Each column is represented by a random vector\n",
    "        col_vecs = np.random.random((num_col, self.vector_size))\n",
    "        # Each row is represented by a random image\n",
    "        row_imgs = np.random.randint(0, 255, (num_row, 64, 64), dtype=np.uint8)\n",
    "\n",
    "        for i in range(num_col):\n",
    "            for j in range(num_row):\n",
    "                self.state_mapping.append({\"vec\": col_vecs[i], \"img\": row_imgs[j].reshape(self.img_size)})\n",
    "\n",
    "    def get_state_mapping(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Uses the state to get the observation mapping.\n",
    "\n",
    "        :return: observation dict {'vec': ..., 'img': ...}\n",
    "        \"\"\"\n",
    "        return self.state_mapping[self.state]\n",
    "\n",
    "    def init_possible_transitions(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the transitions of the environment\n",
    "        The environment exploits the cardinal directions of the grid by noting that\n",
    "        they correspond to simple addition and subtraction from the cell id within the grid\n",
    "\n",
    "        - up => means moving up a row => means subtracting the length of a column\n",
    "        - down => means moving down a row => means adding the length of a column\n",
    "        - left => means moving left by one => means subtracting 1\n",
    "        - right => means moving right by one => means adding 1\n",
    "\n",
    "        Thus one only needs to specify in which states each action is possible\n",
    "        in order to define the transitions of the environment\n",
    "        \"\"\"\n",
    "        self.left_possible = [1, 2, 3, 13, 14, 15]\n",
    "        self.down_possible = [0, 4, 8, 3, 7, 11]\n",
    "        self.right_possible = [0, 1, 2, 12, 13, 14]\n",
    "        self.up_possible = [4, 8, 12, 7, 11, 15]\n",
    "\n",
    "    def step(self, action: Union[int, np.ndarray]):\n",
    "        \"\"\"\n",
    "        Run one timestep of the environment's dynamics. When end of\n",
    "        episode is reached, you are responsible for calling `reset()`\n",
    "        to reset this environment's state.\n",
    "        Accepts an action and returns a tuple (observation, reward, terminated, truncated, info).\n",
    "\n",
    "        :param action:\n",
    "        :return: tuple (observation, reward, terminated, truncated, info).\n",
    "        \"\"\"\n",
    "        if not self.discrete_actions:\n",
    "            action = np.argmax(action)  # type: ignore[assignment]\n",
    "\n",
    "        self.count += 1\n",
    "\n",
    "        prev_state = self.state\n",
    "\n",
    "        reward = -0.1\n",
    "        # define state transition\n",
    "        if self.state in self.left_possible and action == 0:  # left\n",
    "            self.state -= 1\n",
    "        elif self.state in self.down_possible and action == 1:  # down\n",
    "            self.state += self.num_col\n",
    "        elif self.state in self.right_possible and action == 2:  # right\n",
    "            self.state += 1\n",
    "        elif self.state in self.up_possible and action == 3:  # up\n",
    "            self.state -= self.num_col\n",
    "\n",
    "        got_to_end = self.state == self.max_state\n",
    "        reward = 1 if got_to_end else reward\n",
    "        truncated = self.count > self.max_count\n",
    "        terminated = got_to_end\n",
    "\n",
    "        self.log = f\"Went {self.action2str[action]} in state {prev_state}, got to state {self.state}\"\n",
    "\n",
    "        return self.get_state_mapping(), reward, terminated, truncated, {\"got_to_end\": got_to_end}\n",
    "\n",
    "    def render(self, mode: str = \"human\") -> None:\n",
    "        \"\"\"\n",
    "        Prints the log of the environment.\n",
    "\n",
    "        :param mode:\n",
    "        \"\"\"\n",
    "        print(self.log)\n",
    "\n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[Dict[str, np.ndarray], Dict]:\n",
    "        \"\"\"\n",
    "        Resets the environment state and step count and returns reset observation.\n",
    "\n",
    "        :param seed:\n",
    "        :return: observation dict {'vec': ..., 'img': ...}\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            super().reset(seed=seed)\n",
    "        self.count = 0\n",
    "        if not self.random_start:\n",
    "            self.state = 0\n",
    "        else:\n",
    "            self.state = np.random.randint(0, self.max_state)\n",
    "        return self.state_mapping[self.state], {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleMultiObsEnv()\n",
    "\n",
    "obs, _ = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agilerl.algorithms import MATD3\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compile_mode = \"default\"\n",
    "accelerator = None\n",
    "matd3 = MATD3(\n",
    "    observation_spaces=[spaces.Box(0, 1, shape=(3, 32, 32))],\n",
    "    action_spaces=[spaces.Discrete(2)],\n",
    "    one_hot=False,\n",
    "    n_agents=1,\n",
    "    agent_ids=[\"agent_0\"],\n",
    "    max_action=[(1,)],\n",
    "    min_action=[(-1,)],\n",
    "    discrete_actions=True,\n",
    "    device=device,\n",
    "    torch_compiler=compile_mode,\n",
    "    accelerator=accelerator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ AgileRL ============\n",
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import fastrand\n",
    "\n",
    "from agilerl.algorithms.base import RLAlgorithm\n",
    "from agilerl.components.replay_buffer import ReplayBuffer\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.modules.mlp import EvolvableMLP\n",
    "from agilerl.training.train_off_policy import train_off_policy\n",
    "from agilerl.utils.utils import (\n",
    "    create_population,\n",
    "    make_vect_envs,\n",
    "    observation_space_channels_to_first,\n",
    "    print_hyperparams\n",
    ")\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "with open(\"configs/training/ppo.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "INIT_HP = config[\"INIT_HP\"]\n",
    "MUTATION_PARAMS = config[\"MUTATION_PARAMS\"]\n",
    "NET_CONFIG = config[\"NET_CONFIG\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"============ AgileRL ============\")\n",
    "print(f\"DEVICE: {device}\")\n",
    "\n",
    "env = make_vect_envs(INIT_HP[\"ENV_NAME\"], num_envs=INIT_HP[\"NUM_ENVS\"])\n",
    "\n",
    "observation_space = env.single_observation_space\n",
    "action_space = env.single_action_space\n",
    "if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "    observation_space = observation_space_channels_to_first(observation_space)\n",
    "\n",
    "tournament = TournamentSelection(\n",
    "    INIT_HP[\"TOURN_SIZE\"],\n",
    "    INIT_HP[\"ELITISM\"],\n",
    "    INIT_HP[\"POP_SIZE\"],\n",
    "    INIT_HP[\"EVAL_LOOP\"],\n",
    ")\n",
    "\n",
    "def init_objs(INIT_HP, MUTATION_PARAMS, device):\n",
    "    mutations = Mutations(\n",
    "        algo=INIT_HP[\"ALGO\"],\n",
    "        no_mutation=MUTATION_PARAMS[\"NO_MUT\"],\n",
    "        architecture=MUTATION_PARAMS[\"ARCH_MUT\"],\n",
    "        new_layer_prob=MUTATION_PARAMS[\"NEW_LAYER\"],\n",
    "        parameters=MUTATION_PARAMS[\"PARAMS_MUT\"],\n",
    "        activation=MUTATION_PARAMS[\"ACT_MUT\"],\n",
    "        rl_hp=MUTATION_PARAMS[\"RL_HP_MUT\"],\n",
    "        rl_hp_selection=MUTATION_PARAMS[\"RL_HP_SELECTION\"],\n",
    "        mutation_sd=MUTATION_PARAMS[\"MUT_SD\"],\n",
    "        min_lr=MUTATION_PARAMS[\"MIN_LR\"],\n",
    "        max_lr=MUTATION_PARAMS[\"MAX_LR\"],\n",
    "        min_batch_size=MUTATION_PARAMS[\"MAX_BATCH_SIZE\"],\n",
    "        max_batch_size=MUTATION_PARAMS[\"MAX_BATCH_SIZE\"],\n",
    "        min_learn_step=MUTATION_PARAMS[\"MIN_LEARN_STEP\"],\n",
    "        max_learn_step=MUTATION_PARAMS[\"MAX_LEARN_STEP\"],\n",
    "        arch=NET_CONFIG[\"arch\"],\n",
    "        rand_seed=MUTATION_PARAMS[\"RAND_SEED\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    pop = create_population(\n",
    "        algo=INIT_HP[\"ALGO\"],\n",
    "        observation_space=observation_space,\n",
    "        action_space=action_space,\n",
    "        net_config=NET_CONFIG,\n",
    "        INIT_HP=INIT_HP,\n",
    "        actor_network=None,\n",
    "        critic_network=None,\n",
    "        population_size=INIT_HP[\"POP_SIZE\"],\n",
    "        num_envs=INIT_HP[\"NUM_ENVS\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    return pop, mutations\n",
    "\n",
    "def get_mutation_choices(N: int, mutations: Mutations, pop: List[RLAlgorithm]) -> List[List[str]]:\n",
    "    choices = []\n",
    "    pop = mutations.mutation(pop, pre_training_mut=True)\n",
    "    choices.append([ind.mut for ind in pop])\n",
    "\n",
    "    for _ in range(N):\n",
    "        pop = mutations.mutation(pop)\n",
    "        choices.append([ind.mut for ind in pop])\n",
    "    \n",
    "    return choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_choices(mutations, population,  pre_training_mut: bool = False):\n",
    "    mutation_options = mutations.pretraining_mut_options if pre_training_mut else mutations.mut_options\n",
    "    mutation_proba = mutations.pretraining_mut_proba if pre_training_mut else mutations.mut_proba\n",
    "\n",
    "    # Randomly choose mutation for each agent in population from options with\n",
    "    # relative probabilities\n",
    "    mutation_choice = mutations.rng.choice(\n",
    "        mutation_options, len(population), p=mutation_proba\n",
    "    )\n",
    "    choice_mapping = {\n",
    "        mutations.no_mutation: \"None\",\n",
    "        mutations.architecture_mutate: \"arch\",\n",
    "        mutations.parameter_mutation: \"param\",\n",
    "        mutations.activation_mutation: \"act\",\n",
    "        mutations.rl_hyperparam_mutation: \"rl\"\n",
    "    }\n",
    "    return [choice_mapping[choice] for choice in mutation_choice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         6573424 function calls in 11.548 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        3    0.000    0.000   11.574    3.858 interactiveshell.py:3541(run_code)\n",
      "        3    0.000    0.000   11.574    3.858 {built-in method builtins.exec}\n",
      "    10000    0.019    0.000   11.453    0.001 vector_env.py:166(step)\n",
      "    10000    0.484    0.000    5.887    0.001 async_vector_env.py:290(step_wait)\n",
      "    10000    0.238    0.000    5.547    0.001 async_vector_env.py:265(step_async)\n",
      "   160016    0.313    0.000    5.273    0.000 connection.py:202(send)\n",
      "   160016    0.292    0.000    5.090    0.000 connection.py:246(recv)\n",
      "   160016    0.227    0.000    3.726    0.000 connection.py:429(_recv_bytes)\n",
      "   320032    0.519    0.000    3.431    0.000 connection.py:390(_recv)\n",
      "   320032    2.788    0.000    2.788    0.000 {built-in method posix.read}\n",
      "   160016    0.369    0.000    2.624    0.000 reduction.py:48(dumps)\n",
      "   160016    0.243    0.000    2.262    0.000 connection.py:406(_send_bytes)\n",
      "   160016    0.149    0.000    1.945    0.000 connection.py:381(_send)\n",
      "   160016    1.781    0.000    1.781    0.000 {built-in method posix.write}\n",
      "   160016    1.416    0.000    1.467    0.000 {method 'dump' of '_pickle.Pickler' objects}\n",
      "   160016    0.964    0.000    0.964    0.000 {built-in method _pickle.loads}\n",
      "   160016    0.480    0.000    0.749    0.000 reduction.py:38(__init__)\n",
      "   160016    0.238    0.000    0.238    0.000 {method 'update' of 'dict' objects}\n",
      "   160016    0.088    0.000    0.123    0.000 vector_env.py:275(_add_info)\n",
      "   640065    0.094    0.000    0.094    0.000 {built-in method builtins.len}\n",
      "    10000    0.053    0.000    0.091    0.000 multi_discrete.py:84(sample)\n",
      "   320032    0.087    0.000    0.087    0.000 {method 'getbuffer' of '_io.BytesIO' objects}\n",
      "    10001    0.030    0.000    0.079    0.000 copy.py:128(deepcopy)\n",
      "   320032    0.074    0.000    0.074    0.000 connection.py:135(_check_closed)\n",
      "   320032    0.067    0.000    0.067    0.000 {method 'write' of '_io.BytesIO' objects}\n",
      "   800000    0.060    0.000    0.060    0.000 {method 'append' of 'list' objects}\n",
      "   160016    0.052    0.000    0.052    0.000 {built-in method _struct.pack}\n",
      "   160000    0.051    0.000    0.051    0.000 __init__.py:145(_DType_reduce)\n",
      "   160016    0.046    0.000    0.046    0.000 {built-in method _struct.unpack}\n",
      "    10000    0.013    0.000    0.036    0.000 functools.py:904(wrapper)\n",
      "    30000    0.035    0.000    0.035    0.000 {built-in method numpy.array}\n",
      "   160016    0.032    0.000    0.032    0.000 connection.py:143(_check_writable)\n",
      "   160016    0.031    0.000    0.031    0.000 {method 'copy' of 'dict' objects}\n",
      "    10000    0.029    0.000    0.029    0.000 {method 'random' of 'numpy.random._generator.Generator' objects}\n",
      "   160016    0.027    0.000    0.027    0.000 connection.py:139(_check_readable)\n",
      "   160016    0.022    0.000    0.022    0.000 {method 'getvalue' of '_io.BytesIO' objects}\n",
      "    10001    0.018    0.000    0.021    0.000 copy.py:243(_keep_alive)\n",
      "   160016    0.020    0.000    0.020    0.000 {method 'keys' of 'dict' objects}\n",
      "    10001    0.019    0.000    0.019    0.000 {method '__deepcopy__' of 'numpy.ndarray' objects}\n",
      "    10000    0.010    0.000    0.018    0.000 functools.py:818(dispatch)\n",
      "     3202    0.011    0.000    0.015    0.000 vector_env.py:302(_init_info_arrays)\n",
      "    10001    0.005    0.000    0.008    0.000 async_vector_env.py:527(_raise_if_errors)\n",
      "    10000    0.007    0.000    0.007    0.000 weakref.py:414(__getitem__)\n",
      "    10000    0.006    0.000    0.006    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "    10001    0.004    0.000    0.006    0.000 async_vector_env.py:486(_poll)\n",
      "    30003    0.006    0.000    0.006    0.000 async_vector_env.py:521(_assert_is_running)\n",
      "    10000    0.004    0.000    0.005    0.000 spaces.py:170(_iterate_base)\n",
      "    30003    0.005    0.000    0.005    0.000 {built-in method builtins.id}\n",
      "        1    0.000    0.000    0.003    0.003 491525375.py:1(<module>)\n",
      "        1    0.000    0.000    0.003    0.003 vector_env.py:115(reset)\n",
      "    10001    0.003    0.000    0.003    0.000 {built-in method builtins.all}\n",
      "    20005    0.003    0.000    0.003    0.000 {method 'get' of 'dict' objects}\n",
      "     6404    0.003    0.000    0.003    0.000 {built-in method numpy.zeros}\n",
      "    10000    0.002    0.000    0.002    0.000 space.py:66(np_random)\n",
      "    13203    0.002    0.000    0.002    0.000 {built-in method builtins.issubclass}\n",
      "    10007    0.002    0.000    0.002    0.000 {built-in method builtins.getattr}\n",
      "        1    0.000    0.000    0.002    0.002 async_vector_env.py:172(reset_async)\n",
      "    10000    0.002    0.000    0.002    0.000 {built-in method builtins.iter}\n",
      "    10000    0.002    0.000    0.002    0.000 {built-in method _abc.get_cache_token}\n",
      "        1    0.000    0.000    0.002    0.002 async_vector_env.py:215(reset_wait)\n",
      "        1    0.000    0.000    0.001    0.001 async_vector_env.py:249(<listcomp>)\n",
      "        3    0.000    0.000    0.000    0.000 codeop.py:120(__call__)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "        3    0.000    0.000    0.000    0.000 contextlib.py:299(helper)\n",
      "        3    0.000    0.000    0.000    0.000 traitlets.py:676(__get__)\n",
      "        3    0.000    0.000    0.000    0.000 contextlib.py:104(__init__)\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "        3    0.000    0.000    0.000    0.000 contextlib.py:132(__enter__)\n",
      "        3    0.000    0.000    0.000    0.000 contextlib.py:141(__exit__)\n",
      "        6    0.000    0.000    0.000    0.000 compilerop.py:180(extra_flags)\n",
      "        3    0.000    0.000    0.000    0.000 traitlets.py:629(get)\n",
      "        3    0.000    0.000    0.000    0.000 interactiveshell.py:3493(compare)\n",
      "        3    0.000    0.000    0.000    0.000 interactiveshell.py:1277(user_global_ns)\n",
      "        1    0.000    0.000    0.000    0.000 async_vector_env.py:194(<listcomp>)\n",
      "        6    0.000    0.000    0.000    0.000 typing.py:2287(cast)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Profile step method\n",
    "import cProfile\n",
    "\n",
    "steps = 10000\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "obs = env.reset()\n",
    "for _ in range(steps):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "profiler.disable()\n",
    "profiler.print_stats(sort=\"cumtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<agilerl.algorithms.dqn.DQN at 0x7fe7ad5f8450>,\n",
       " <agilerl.algorithms.dqn.DQN at 0x7fe7af4e78d0>,\n",
       " <agilerl.algorithms.dqn.DQN at 0x7fe7ae9ea450>,\n",
       " <agilerl.algorithms.dqn.DQN at 0x7fe7ae9d6e90>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_pop, pop_fitnesses = train_off_policy(\n",
    "    env,\n",
    "    INIT_HP[\"ENV_NAME\"],\n",
    "    INIT_HP[\"ALGO\"],\n",
    "    agent_pop,\n",
    "    memory=memory,\n",
    "    INIT_HP=INIT_HP,\n",
    "    MUT_P=MUTATION_PARAMS,\n",
    "    swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "    max_steps=INIT_HP[\"MAX_STEPS\"],\n",
    "    evo_steps=INIT_HP[\"EVO_STEPS\"],\n",
    "    eval_steps=INIT_HP[\"EVAL_STEPS\"],\n",
    "    eval_loop=INIT_HP[\"EVAL_LOOP\"],\n",
    "    learning_delay=INIT_HP[\"LEARNING_DELAY\"],\n",
    "    eps_start=INIT_HP[\"EPS_START\"] if \"EPS_START\" in INIT_HP else 1.0,\n",
    "    eps_end=INIT_HP[\"EPS_END\"] if \"EPS_END\" in INIT_HP else 0.01,\n",
    "    eps_decay=INIT_HP[\"EPS_DECAY\"] if \"EPS_DECAY\" in INIT_HP else 0.999,\n",
    "    target=INIT_HP[\"TARGET_SCORE\"],\n",
    "    tournament=tournament,\n",
    "    mutation=mutations,\n",
    "    wb=INIT_HP[\"WANDB\"],\n",
    ")\n",
    "\n",
    "print_hyperparams(trained_pop)\n",
    "# plot_population_score(trained_pop)\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Racecar Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agilerl.modules.cnn import EvolvableCNN\n",
    "from agilerl.algorithms.ppo import PPO\n",
    "from accelerate import Accelerator\n",
    "\n",
    "observation_space = spaces.Box(low=0, high=255, shape=(3, 32, 32), dtype=np.uint8)\n",
    "action_space = spaces.Discrete(2)\n",
    "one_hot = False\n",
    "net_config_cnn = {\n",
    "    \"arch\": \"cnn\",\n",
    "    \"hidden_size\": [8],\n",
    "    \"channel_size\": [3],\n",
    "    \"kernel_size\": [3],\n",
    "    \"stride_size\": [1],\n",
    "    \"normalize\": False,\n",
    "}\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "mut = None\n",
    "action_std_init = 0.6\n",
    "clip_coef = 0.2\n",
    "ent_coef = 0.01\n",
    "vf_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "target_kl = None\n",
    "update_epochs = 4\n",
    "actor_network = None\n",
    "critic_network = None\n",
    "accelerator = Accelerator()\n",
    "wrap = True\n",
    "\n",
    "ppo = PPO(\n",
    "    observation_space=observation_space,\n",
    "    action_space=action_space,\n",
    "    one_hot=one_hot,\n",
    "    discrete_actions=True,\n",
    "    net_config=net_config_cnn,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    gamma=gamma,\n",
    "    gae_lambda=gae_lambda,\n",
    "    mut=mut,\n",
    "    action_std_init=action_std_init,\n",
    "    clip_coef=clip_coef,\n",
    "    ent_coef=ent_coef,\n",
    "    vf_coef=vf_coef,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    target_kl=target_kl,\n",
    "    update_epochs=update_epochs,\n",
    "    actor_network=actor_network,\n",
    "    critic_network=critic_network,\n",
    "    accelerator=accelerator,\n",
    "    wrap=wrap\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{AcceleratedOptimizer (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.0001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " \n",
       " Parameter Group 1\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.0001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ): ['critic', 'actor']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.optim_to_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['actor', 'critic', 'optimizer'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.evolvable_attributes().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo._identify_param_to_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actor'}\n",
      "{'actor'}\n",
      "{'actor'}\n",
      "{'actor'}\n",
      "{'actor'}\n",
      "{'actor'}\n",
      "{'critic'}\n",
      "{'critic'}\n",
      "{'critic'}\n",
      "{'critic'}\n",
      "{'critic'}\n",
      "{'critic'}\n"
     ]
    }
   ],
   "source": [
    "optimzer = ppo.optimizer\n",
    "\n",
    "for param_group in optimzer.param_groups:\n",
    "    for param in param_group[\"params\"]:\n",
    "        print(param._evol_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On-policy\n",
    "if swap_channels:\n",
    "    state = np.moveaxis(state, [-1], [-3])\n",
    "\n",
    "# Multi-agent\n",
    "if swap_channels:\n",
    "    if not is_vectorised:\n",
    "        state = {\n",
    "            agent_id: np.moveaxis(np.expand_dims(s, 0), [-1], [-3])\n",
    "            for agent_id, s in state.items()\n",
    "        }\n",
    "    else:\n",
    "        state = {\n",
    "            agent_id: np.moveaxis(s, [-1], [-3])\n",
    "            for agent_id, s in state.items()\n",
    "        }\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agilerl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
