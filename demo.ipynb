{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaimesabal/.pyenv/versions/agilerl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from agilerl.networks.evolvable_cnn import EvolvableCNN\n",
    "from agilerl.networks.evolvable_composed import EvolvableComposed\n",
    "from agilerl.networks.evolvable_mlp import EvolvableMLP\n",
    "from agilerl.hpo.mutation import Mutations, get_return_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining an EvolvableComposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvolvableComposed(\n",
       "  (feature_net): ModuleDict(\n",
       "    (image1): EvolvableCNN(\n",
       "      (feature_net): Sequential(\n",
       "        (feature_conv_layer_0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (feature_activation_0): ReLU()\n",
       "        (feature_conv_layer_1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (feature_activation_1): ReLU()\n",
       "        (feature_conv_layer_2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (feature_activation_2): ReLU()\n",
       "        (feature_flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "        (feature_linear_output): Linear(in_features=1984032, out_features=10, bias=True)\n",
       "        (feature_output_activation): ReLU()\n",
       "      )\n",
       "      (value_net): Sequential(\n",
       "        (feature_head_linear_layer_0): Linear(in_features=10, out_features=64, bias=True)\n",
       "        (feature_head_activation_0): ReLU()\n",
       "        (feature_head_linear_layer_1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (feature_head_activation_1): ReLU()\n",
       "        (feature_head_linear_layer_output): Linear(in_features=64, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (image2): EvolvableCNN(\n",
       "      (feature_net): Sequential(\n",
       "        (feature_conv_layer_0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (feature_activation_0): ReLU()\n",
       "        (feature_conv_layer_1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (feature_activation_1): ReLU()\n",
       "        (feature_conv_layer_2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (feature_activation_2): ReLU()\n",
       "        (feature_flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "        (feature_linear_output): Linear(in_features=1984032, out_features=10, bias=True)\n",
       "        (feature_output_activation): ReLU()\n",
       "      )\n",
       "      (value_net): Sequential(\n",
       "        (feature_head_linear_layer_0): Linear(in_features=10, out_features=64, bias=True)\n",
       "        (feature_head_activation_0): ReLU()\n",
       "        (feature_head_linear_layer_1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (feature_head_activation_1): ReLU()\n",
       "        (feature_head_linear_layer_output): Linear(in_features=64, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (image3): EvolvableCNN(\n",
       "      (feature_net): Sequential(\n",
       "        (feature_conv_layer_0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (feature_activation_0): ReLU()\n",
       "        (feature_conv_layer_1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (feature_activation_1): ReLU()\n",
       "        (feature_conv_layer_2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (feature_activation_2): ReLU()\n",
       "        (feature_flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "        (feature_linear_output): Linear(in_features=1984032, out_features=10, bias=True)\n",
       "        (feature_output_activation): ReLU()\n",
       "      )\n",
       "      (value_net): Sequential(\n",
       "        (feature_head_linear_layer_0): Linear(in_features=10, out_features=64, bias=True)\n",
       "        (feature_head_activation_0): ReLU()\n",
       "        (feature_head_linear_layer_1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (feature_head_activation_1): ReLU()\n",
       "        (feature_head_linear_layer_output): Linear(in_features=64, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (value_net): EvolvableMLP(\n",
       "    (feature_net): Sequential(\n",
       "      (mlp_linear_layer_0): Linear(in_features=36, out_features=64, bias=True)\n",
       "      (mlp_activation_0): ReLU()\n",
       "      (mlp_linear_layer_1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (mlp_activation_1): ReLU()\n",
       "      (mlp_linear_layer_output): Linear(in_features=64, out_features=51, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (advantage_net): EvolvableMLP(\n",
       "    (feature_net): Sequential(\n",
       "      (mlp_linear_layer_0): Linear(in_features=36, out_features=64, bias=True)\n",
       "      (mlp_activation_0): ReLU()\n",
       "      (mlp_linear_layer_1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (mlp_activation_1): ReLU()\n",
       "      (mlp_linear_layer_output): Linear(in_features=64, out_features=510, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "# Create a sample Dict observation space with 3 image spaces and 2 vector spaces\n",
    "observation_space = spaces.Dict({\n",
    "    \"image1\": spaces.Box(low=0, high=255, shape=(3, 255, 255), dtype=np.uint8),\n",
    "    \"image2\": spaces.Box(low=0, high=255, shape=(3, 255, 255), dtype=np.uint8),\n",
    "    \"image3\": spaces.Box(low=0, high=255, shape=(3, 255, 255), dtype=np.uint8),\n",
    "    \"vector1\": spaces.Box(low=-1, high=1, shape=(3,), dtype=np.float32),\n",
    "    \"vector2\": spaces.Box(low=-1, high=1, shape=(3,), dtype=np.float32),\n",
    "})\n",
    "\n",
    "latent_dim = 10\n",
    "num_outputs = 10\n",
    "num_atoms = 51\n",
    "channel_size = (8, 16, 32)\n",
    "kernel_size = (3, 3, 3)\n",
    "stride_size = (1, 1, 1)\n",
    "hidden_size = [64, 64]\n",
    "v_min = -10\n",
    "v_max = 10\n",
    "support = torch.linspace(v_min, v_max, num_atoms)\n",
    "net = EvolvableComposed(observation_space, channel_size, kernel_size, stride_size, hidden_size, latent_dim, num_outputs, rainbow=True, num_atoms=num_atoms, support=support)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_return_type(net.add_cnn_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probe Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "class SimpleMultiObsEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Base class for GridWorld-based MultiObs Environments 4x4  grid world.\n",
    "\n",
    "    .. code-block:: text\n",
    "\n",
    "        ____________\n",
    "       | 0  1  2   3|\n",
    "       | 4|¯5¯¯6¯| 7|\n",
    "       | 8|_9_10_|11|\n",
    "       |12 13  14 15|\n",
    "       ¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
    "\n",
    "    start is 0\n",
    "    states 5, 6, 9, and 10 are blocked\n",
    "    goal is 15\n",
    "    actions are = [left, down, right, up]\n",
    "\n",
    "    simple linear state env of 15 states but encoded with a vector and an image observation:\n",
    "    each column is represented by a random vector and each row is\n",
    "    represented by a random image, both sampled once at creation time.\n",
    "\n",
    "    :param num_col: Number of columns in the grid\n",
    "    :param num_row: Number of rows in the grid\n",
    "    :param random_start: If true, agent starts in random position\n",
    "    :param channel_last: If true, the image will be channel last, else it will be channel first\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_col: int = 4,\n",
    "        num_row: int = 4,\n",
    "        random_start: bool = True,\n",
    "        discrete_actions: bool = True,\n",
    "        channel_last: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vector_size = 5\n",
    "        if channel_last:\n",
    "            self.img_size = [64, 64, 1]\n",
    "        else:\n",
    "            self.img_size = [1, 64, 64]\n",
    "\n",
    "        self.random_start = random_start\n",
    "        self.discrete_actions = discrete_actions\n",
    "        if discrete_actions:\n",
    "            self.action_space = spaces.Discrete(4)\n",
    "        else:\n",
    "            self.action_space = spaces.Box(0, 1, (4,))\n",
    "\n",
    "        self.observation_space = spaces.Dict(\n",
    "            spaces={\n",
    "                \"vec\": spaces.Box(0, 1, (self.vector_size,), dtype=np.float64),\n",
    "                \"img\": spaces.Box(0, 255, self.img_size, dtype=np.uint8),\n",
    "            }\n",
    "        )\n",
    "        self.count = 0\n",
    "        # Timeout\n",
    "        self.max_count = 100\n",
    "        self.log = \"\"\n",
    "        self.state = 0\n",
    "        self.action2str = [\"left\", \"down\", \"right\", \"up\"]\n",
    "        self.init_possible_transitions()\n",
    "\n",
    "        self.num_col = num_col\n",
    "        self.state_mapping: List[Dict[str, np.ndarray]] = []\n",
    "        self.init_state_mapping(num_col, num_row)\n",
    "\n",
    "        self.max_state = len(self.state_mapping) - 1\n",
    "\n",
    "    def init_state_mapping(self, num_col: int, num_row: int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the state_mapping array which holds the observation values for each state\n",
    "\n",
    "        :param num_col: Number of columns.\n",
    "        :param num_row: Number of rows.\n",
    "        \"\"\"\n",
    "        # Each column is represented by a random vector\n",
    "        col_vecs = np.random.random((num_col, self.vector_size))\n",
    "        # Each row is represented by a random image\n",
    "        row_imgs = np.random.randint(0, 255, (num_row, 64, 64), dtype=np.uint8)\n",
    "\n",
    "        for i in range(num_col):\n",
    "            for j in range(num_row):\n",
    "                self.state_mapping.append({\"vec\": col_vecs[i], \"img\": row_imgs[j].reshape(self.img_size)})\n",
    "\n",
    "    def get_state_mapping(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Uses the state to get the observation mapping.\n",
    "\n",
    "        :return: observation dict {'vec': ..., 'img': ...}\n",
    "        \"\"\"\n",
    "        return self.state_mapping[self.state]\n",
    "\n",
    "    def init_possible_transitions(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the transitions of the environment\n",
    "        The environment exploits the cardinal directions of the grid by noting that\n",
    "        they correspond to simple addition and subtraction from the cell id within the grid\n",
    "\n",
    "        - up => means moving up a row => means subtracting the length of a column\n",
    "        - down => means moving down a row => means adding the length of a column\n",
    "        - left => means moving left by one => means subtracting 1\n",
    "        - right => means moving right by one => means adding 1\n",
    "\n",
    "        Thus one only needs to specify in which states each action is possible\n",
    "        in order to define the transitions of the environment\n",
    "        \"\"\"\n",
    "        self.left_possible = [1, 2, 3, 13, 14, 15]\n",
    "        self.down_possible = [0, 4, 8, 3, 7, 11]\n",
    "        self.right_possible = [0, 1, 2, 12, 13, 14]\n",
    "        self.up_possible = [4, 8, 12, 7, 11, 15]\n",
    "\n",
    "    def step(self, action: Union[int, np.ndarray]):\n",
    "        \"\"\"\n",
    "        Run one timestep of the environment's dynamics. When end of\n",
    "        episode is reached, you are responsible for calling `reset()`\n",
    "        to reset this environment's state.\n",
    "        Accepts an action and returns a tuple (observation, reward, terminated, truncated, info).\n",
    "\n",
    "        :param action:\n",
    "        :return: tuple (observation, reward, terminated, truncated, info).\n",
    "        \"\"\"\n",
    "        if not self.discrete_actions:\n",
    "            action = np.argmax(action)  # type: ignore[assignment]\n",
    "\n",
    "        self.count += 1\n",
    "\n",
    "        prev_state = self.state\n",
    "\n",
    "        reward = -0.1\n",
    "        # define state transition\n",
    "        if self.state in self.left_possible and action == 0:  # left\n",
    "            self.state -= 1\n",
    "        elif self.state in self.down_possible and action == 1:  # down\n",
    "            self.state += self.num_col\n",
    "        elif self.state in self.right_possible and action == 2:  # right\n",
    "            self.state += 1\n",
    "        elif self.state in self.up_possible and action == 3:  # up\n",
    "            self.state -= self.num_col\n",
    "\n",
    "        got_to_end = self.state == self.max_state\n",
    "        reward = 1 if got_to_end else reward\n",
    "        truncated = self.count > self.max_count\n",
    "        terminated = got_to_end\n",
    "\n",
    "        self.log = f\"Went {self.action2str[action]} in state {prev_state}, got to state {self.state}\"\n",
    "\n",
    "        return self.get_state_mapping(), reward, terminated, truncated, {\"got_to_end\": got_to_end}\n",
    "\n",
    "    def render(self, mode: str = \"human\") -> None:\n",
    "        \"\"\"\n",
    "        Prints the log of the environment.\n",
    "\n",
    "        :param mode:\n",
    "        \"\"\"\n",
    "        print(self.log)\n",
    "\n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[Dict[str, np.ndarray], Dict]:\n",
    "        \"\"\"\n",
    "        Resets the environment state and step count and returns reset observation.\n",
    "\n",
    "        :param seed:\n",
    "        :return: observation dict {'vec': ..., 'img': ...}\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            super().reset(seed=seed)\n",
    "        self.count = 0\n",
    "        if not self.random_start:\n",
    "            self.state = 0\n",
    "        else:\n",
    "            self.state = np.random.randint(0, self.max_state)\n",
    "        return self.state_mapping[self.state], {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleMultiObsEnv()\n",
    "\n",
    "obs, _ = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agilerl.algorithms import MATD3\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compile_mode = \"default\"\n",
    "accelerator = None\n",
    "matd3 = MATD3(\n",
    "    observation_spaces=[spaces.Box(0, 1, shape=(3, 32, 32))],\n",
    "    action_spaces=[spaces.Discrete(2)],\n",
    "    one_hot=False,\n",
    "    n_agents=1,\n",
    "    agent_ids=[\"agent_0\"],\n",
    "    max_action=[(1,)],\n",
    "    min_action=[(-1,)],\n",
    "    discrete_actions=True,\n",
    "    device=device,\n",
    "    torch_compiler=compile_mode,\n",
    "    accelerator=accelerator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agilerl.algorithms.base import RLAlgorithm\n",
    "from agilerl.components.replay_buffer import ReplayBuffer\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.networks.evolvable_mlp import EvolvableMLP\n",
    "from agilerl.training.train_off_policy import train_off_policy\n",
    "from agilerl.utils.utils import (\n",
    "    create_population,\n",
    "    make_vect_envs,\n",
    "    observation_space_channels_to_first,\n",
    "    print_hyperparams\n",
    ")\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open(\"configs/training/dqn.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "INIT_HP = config[\"INIT_HP\"]\n",
    "MUTATION_PARAMS = config[\"MUTATION_PARAMS\"]\n",
    "NET_CONFIG = config[\"NET_CONFIG\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"============ AgileRL ============\")\n",
    "print(f\"DEVICE: {device}\")\n",
    "\n",
    "env = make_vect_envs(INIT_HP[\"ENV_NAME\"], num_envs=INIT_HP[\"NUM_ENVS\"])\n",
    "\n",
    "observation_space = env.single_observation_space\n",
    "action_space = env.single_action_space\n",
    "if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "    observation_space = observation_space_channels_to_first(observation_space)\n",
    "\n",
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "memory = ReplayBuffer(\n",
    "    memory_size=INIT_HP[\"MEMORY_SIZE\"], field_names=field_names, device=device\n",
    ")\n",
    "tournament = TournamentSelection(\n",
    "    INIT_HP[\"TOURN_SIZE\"],\n",
    "    INIT_HP[\"ELITISM\"],\n",
    "    INIT_HP[\"POP_SIZE\"],\n",
    "    INIT_HP[\"EVAL_LOOP\"],\n",
    ")\n",
    "mutations = Mutations(\n",
    "    algo=INIT_HP[\"ALGO\"],\n",
    "    no_mutation=MUTATION_PARAMS[\"NO_MUT\"],\n",
    "    architecture=MUTATION_PARAMS[\"ARCH_MUT\"],\n",
    "    new_layer_prob=MUTATION_PARAMS[\"NEW_LAYER\"],\n",
    "    parameters=MUTATION_PARAMS[\"PARAMS_MUT\"],\n",
    "    activation=MUTATION_PARAMS[\"ACT_MUT\"],\n",
    "    rl_hp=MUTATION_PARAMS[\"RL_HP_MUT\"],\n",
    "    rl_hp_selection=MUTATION_PARAMS[\"RL_HP_SELECTION\"],\n",
    "    mutation_sd=MUTATION_PARAMS[\"MUT_SD\"],\n",
    "    min_lr=MUTATION_PARAMS[\"MIN_LR\"],\n",
    "    max_lr=MUTATION_PARAMS[\"MAX_LR\"],\n",
    "    min_batch_size=MUTATION_PARAMS[\"MAX_BATCH_SIZE\"],\n",
    "    max_batch_size=MUTATION_PARAMS[\"MAX_BATCH_SIZE\"],\n",
    "    min_learn_step=MUTATION_PARAMS[\"MIN_LEARN_STEP\"],\n",
    "    max_learn_step=MUTATION_PARAMS[\"MAX_LEARN_STEP\"],\n",
    "    arch=NET_CONFIG[\"arch\"],\n",
    "    rand_seed=MUTATION_PARAMS[\"RAND_SEED\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "state_dim = RLAlgorithm.get_state_dim(observation_space)\n",
    "action_dim = RLAlgorithm.get_action_dim(action_space)\n",
    "\n",
    "agent_pop = create_population(\n",
    "    algo=INIT_HP[\"ALGO\"],\n",
    "    observation_space=observation_space,\n",
    "    action_space=action_space,\n",
    "    net_config=NET_CONFIG,\n",
    "    INIT_HP=INIT_HP,\n",
    "    actor_network=None,\n",
    "    critic_network=None,\n",
    "    population_size=INIT_HP[\"POP_SIZE\"],\n",
    "    num_envs=INIT_HP[\"NUM_ENVS\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "trained_pop, pop_fitnesses = train_off_policy(\n",
    "    env,\n",
    "    INIT_HP[\"ENV_NAME\"],\n",
    "    INIT_HP[\"ALGO\"],\n",
    "    agent_pop,\n",
    "    memory=memory,\n",
    "    INIT_HP=INIT_HP,\n",
    "    MUT_P=MUTATION_PARAMS,\n",
    "    swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "    max_steps=INIT_HP[\"MAX_STEPS\"],\n",
    "    evo_steps=INIT_HP[\"EVO_STEPS\"],\n",
    "    eval_steps=INIT_HP[\"EVAL_STEPS\"],\n",
    "    eval_loop=INIT_HP[\"EVAL_LOOP\"],\n",
    "    learning_delay=INIT_HP[\"LEARNING_DELAY\"],\n",
    "    eps_start=INIT_HP[\"EPS_START\"] if \"EPS_START\" in INIT_HP else 1.0,\n",
    "    eps_end=INIT_HP[\"EPS_END\"] if \"EPS_END\" in INIT_HP else 0.01,\n",
    "    eps_decay=INIT_HP[\"EPS_DECAY\"] if \"EPS_DECAY\" in INIT_HP else 0.999,\n",
    "    target=INIT_HP[\"TARGET_SCORE\"],\n",
    "    tournament=tournament,\n",
    "    mutation=mutations,\n",
    "    wb=INIT_HP[\"WANDB\"],\n",
    ")\n",
    "\n",
    "print_hyperparams(trained_pop)\n",
    "# plot_population_score(trained_pop)\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "ind = agent_pop[0]\n",
    "\n",
    "# Get all attributes of the current object\n",
    "attributes = inspect.getmembers(ind, lambda a: not (inspect.isroutine(a)))\n",
    "\n",
    "# Exclude attributes that are EvolvableModule's or Optimizer's (also check for nested \n",
    "# module-related attributes for multi-agent algorithms)\n",
    "exclude = list(ind.evolvable_attributes().keys())\n",
    "\n",
    "# Exclude private and built-in attributes\n",
    "attributes = [\n",
    "    a for a in attributes if not (a[0].startswith(\"_\") or a[0].endswith(\"_\"))\n",
    "]\n",
    "\n",
    "# If input_args_only is True, only include attributes that are \n",
    "# input arguments to the constructor\n",
    "constructor_params = inspect.signature(ind.__init__).parameters.keys()\n",
    "attributes = {\n",
    "    k: v\n",
    "    for k, v in attributes\n",
    "    if k not in exclude and k in constructor_params\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Racecar Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agilerl.networks.evolvable_cnn import EvolvableCNN\n",
    "from agilerl.algorithms.ppo import PPO\n",
    "from accelerate import Accelerator\n",
    "\n",
    "observation_space = spaces.Box(low=0, high=255, shape=(3, 32, 32), dtype=np.uint8)\n",
    "action_space = spaces.Discrete(2)\n",
    "one_hot = False\n",
    "net_config_cnn = {\n",
    "    \"arch\": \"cnn\",\n",
    "    \"hidden_size\": [8],\n",
    "    \"channel_size\": [3],\n",
    "    \"kernel_size\": [3],\n",
    "    \"stride_size\": [1],\n",
    "    \"normalize\": False,\n",
    "}\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "mut = None\n",
    "action_std_init = 0.6\n",
    "clip_coef = 0.2\n",
    "ent_coef = 0.01\n",
    "vf_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "target_kl = None\n",
    "update_epochs = 4\n",
    "actor_network = None\n",
    "critic_network = None\n",
    "accelerator = Accelerator()\n",
    "wrap = True\n",
    "\n",
    "ppo = PPO(\n",
    "    observation_space=observation_space,\n",
    "    action_space=action_space,\n",
    "    one_hot=one_hot,\n",
    "    discrete_actions=True,\n",
    "    net_config=net_config_cnn,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    gamma=gamma,\n",
    "    gae_lambda=gae_lambda,\n",
    "    mut=mut,\n",
    "    action_std_init=action_std_init,\n",
    "    clip_coef=clip_coef,\n",
    "    ent_coef=ent_coef,\n",
    "    vf_coef=vf_coef,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    target_kl=target_kl,\n",
    "    update_epochs=update_epochs,\n",
    "    actor_network=actor_network,\n",
    "    critic_network=critic_network,\n",
    "    accelerator=accelerator,\n",
    "    wrap=wrap\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{AcceleratedOptimizer (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.0001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " \n",
       " Parameter Group 1\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.0001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ): ['critic', 'actor']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.optim_to_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['actor', 'critic', 'optimizer'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.evolvable_attributes().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo._identify_param_to_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actor'}\n",
      "{'actor'}\n",
      "{'actor'}\n",
      "{'actor'}\n",
      "{'actor'}\n",
      "{'actor'}\n",
      "{'critic'}\n",
      "{'critic'}\n",
      "{'critic'}\n",
      "{'critic'}\n",
      "{'critic'}\n",
      "{'critic'}\n"
     ]
    }
   ],
   "source": [
    "optimzer = ppo.optimizer\n",
    "\n",
    "for param_group in optimzer.param_groups:\n",
    "    for param in param_group[\"params\"]:\n",
    "        print(param._evol_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On-policy\n",
    "if swap_channels:\n",
    "    state = np.moveaxis(state, [-1], [-3])\n",
    "\n",
    "# Multi-agent\n",
    "if swap_channels:\n",
    "    if not is_vectorised:\n",
    "        state = {\n",
    "            agent_id: np.moveaxis(np.expand_dims(s, 0), [-1], [-3])\n",
    "            for agent_id, s in state.items()\n",
    "        }\n",
    "    else:\n",
    "        state = {\n",
    "            agent_id: np.moveaxis(s, [-1], [-3])\n",
    "            for agent_id, s in state.items()\n",
    "        }\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agilerl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
