
# Generated by CodiumAI
import torch
import nntplib
from agilerl.networks.make_evolvable import MakeEvolvable
import numpy as np
import torch.nn as nn


import pytest

class TestMakeEvolvable:

    # Tests that the forward method can be called with a valid input tensor
    def test_forward_with_valid_input_tensor(self):
        input_tensor = torch.randn(1, 10)
        network = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2))
        evolvable = MakeEvolvable(network, input_tensor)
        output = evolvable.forward(input_tensor)
        assert isinstance(output, torch.Tensor)

    # Tests that the count_parameters method returns the correct number of parameters
    def test_count_parameters_returns_correct_number_of_parameters(self):
        input_tensor = torch.randn(1, 10)
        network = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2))
        evolvable = MakeEvolvable(network, input_tensor)
        num_params = evolvable.count_parameters()
        assert num_params == 67

    # Tests that the extract_parameters method returns a numpy array of the correct shape
    def test_extract_parameters_returns_numpy_array_of_correct_shape(self):
        input_tensor = torch.randn(1, 10)
        network = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2))
        evolvable = MakeEvolvable(network, input_tensor)
        params = evolvable.extract_parameters()
        assert isinstance(params, np.ndarray)
        assert params.shape == (67,)

    # Tests that the inject_parameters method updates the parameters correctly
    def test_inject_parameters_updates_parameters(self):
        input_tensor = torch.randn(1, 10)
        network = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2))
        evolvable = MakeEvolvable(network, input_tensor)
        params = evolvable.extract_parameters()
        new_params = np.ones_like(params)
        evolvable.inject_parameters(new_params)
        updated_params = evolvable.extract_parameters()
        assert np.array_equal(updated_params, new_params)

    # Tests that the add_mlp_layer method adds a new hidden layer to the network
    def test_add_mlp_layer_adds_new_hidden_layer(self): #
        input_tensor = torch.randn(1, 10)
        network = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2))
        evolvable = MakeEvolvable(network, input_tensor)
        num_hidden_layers = len(evolvable.hidden_size)
        evolvable.add_mlp_layer()
        new_num_hidden_layers = len(evolvable.hidden_size)
        assert new_num_hidden_layers == num_hidden_layers + 1

    # Tests that the remove_layer method removes the last hidden layer from the network
    def test_remove_layer_removes_last_hidden_layer(self): #
        input_tensor = torch.randn(1, 10)
        network = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2), nn.Linear(2, 3))
        evolvable = MakeEvolvable(network, input_tensor)
        num_hidden_layers = len(evolvable.hidden_size)
        evolvable.remove_layer()
        new_num_hidden_layers = len(evolvable.hidden_size)
        assert new_num_hidden_layers == num_hidden_layers - 1
        assert len(evolvable.hidden_size) >= 1

    # Tests that an instance of MakeEvolvable cannot be created with an input tensor that has the wrong shape
    def test_create_instance_with_wrong_input_shape(self):
        input_tensor = torch.randn(1, 5)  # Wrong shape
        network = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2))
        with pytest.raises(RuntimeError):
            evolvable = MakeEvolvable(network, input_tensor)

    # Test that calling the add_node method with an out of range hidden_layer argument does not raise an error and adjusts the hidden_layer value
    def test_add_node_with_invalid_hidden_layer(self):
        input_tensor = torch.randn(1, 10)
        network = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2))
        evolvable = MakeEvolvable(network, input_tensor)
        hidden_layers = len(evolvable.hidden_size)
        evolvable.add_node(hidden_layer=hidden_layers + 1)
        assert len(evolvable.hidden_size) == hidden_layers
        evolvable.add_node(hidden_layer=hidden_layers + 10)
        assert len(evolvable.hidden_size) == hidden_layers

    # Tests that the extract_grad method returns a numpy array of zeros when no gradients have been computed
    def test_extract_grad_with_no_gradients(self):
        input_tensor = torch.randn(1, 10)
        network = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2))
        evolvable = MakeEvolvable(network, input_tensor)
        output = evolvable(input_tensor)
        loss = output.sum()
        loss.backward()
        grad = evolvable.extract_grad()
        assert isinstance(grad, np.ndarray), 'Output should be a numpy array'
        assert np.all(grad == 0), 'All gradients should be zero as no training has been done'

    # Tests that calling the inject_parameters method with a numpy array of the wrong shape raises a ValueError
    def test_inject_parameters_with_wrong_shape(self):
        input_tensor = torch.randn(1, 10)
        network = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2))
        evolvable = MakeEvolvable(network, input_tensor)
        pvec = np.zeros(20, np.float32)  # Create a numpy array with wrong shape
        with pytest.raises(ValueError):
            evolvable.inject_parameters(pvec)

    # Test that the clone method creates a new object with the same parameters as the original object
    def test_clone_method(self):
        input_tensor = torch.randn(1, 10)
        network = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2))
        evolvable = MakeEvolvable(network, input_tensor)
        clone = evolvable.clone()
        assert isinstance(clone, MakeEvolvable), 'Clone is not instance of MakeEvolvable'
        assert evolvable.init_dict == clone.init_dict, 'Initialization dictionaries do not match'
        assert evolvable.count_parameters() == clone.count_parameters(), 'Parameter counts do not match'
        assert np.array_equal(evolvable.extract_parameters(), clone.extract_parameters()), 'Extracted parameters do not match'
        assert np.array_equal(evolvable.extract_grad(), clone.extract_grad()), 'Extracted gradients do not match'

    # Test that a node is removed from the specified hidden layer when calling the remove_node method
    def test_remove_node(self):
        input_tensor = torch.randn(1, 10)
        network = nn.Sequential(nn.Linear(10, 128), nn.ReLU(), nn.Linear(128, 2))
        evolvable = MakeEvolvable(network, input_tensor)
        hidden_layer = 0
        initial_hidden_size = evolvable.hidden_size[hidden_layer]
        numb_new_nodes = 4
        evolvable.remove_node(hidden_layer=hidden_layer, numb_new_nodes=numb_new_nodes)
        assert evolvable.hidden_size[hidden_layer] == initial_hidden_size - numb_new_nodes

    # Tests that the add_node method adds a new node to the specified hidden layer
    def test_add_node_method(self):
        input_tensor = torch.randn(1, 10)
        network = nn.Sequential(nn.Linear(10, 64), 
                                nn.ReLU(), 
                                nn.Linear(64, 128),
                                nn.ReLU(), 
                                nn.Linear(128,5))
        evolvable = MakeEvolvable(network, input_tensor)
        hidden_layer = 1
        numb_new_nodes = 16
        evolvable.add_node(hidden_layer=hidden_layer, numb_new_nodes=numb_new_nodes)
        assert len(evolvable.hidden_size) == 2
        assert evolvable.hidden_size[hidden_layer] == 128 + 16
        assert isinstance(evolvable.net, nn.Sequential)
        assert isinstance(evolvable.net.linear_layer_1, nn.Linear)
        assert evolvable.net.linear_layer_1.in_features == 64
        assert evolvable.net.linear_layer_1.out_features == 144


    # Tests that the extract_grad method returns a numpy array of zeros when called before any gradients have been computed
    def test_extract_grad_with_no_gradients(self):
        input_tensor = torch.randn(1, 10)
        network = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2))
        evolvable = MakeEvolvable(network, input_tensor)
        grad = evolvable.extract_grad()
        assert isinstance(grad, np.ndarray)
        assert np.all(grad == 0)

    def test_get_activation(self):
        input_tensor = torch.randn(1, 10)
        network = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2))
        evolvable = MakeEvolvable(network, input_tensor)
        activation_list = [
            'tanh',
            'linear',
            'relu',
            'elu',
            'softsign',
            'sigmoid',
            'gumbel_softmax',
            'softplus',
            'softmax',
            'lrelu',
            'prelu',
            'gelu'
        ]
        for name in activation_list:
            activation_function = evolvable.get_activation(name)
            assert isinstance(activation_function, nn.Module)

